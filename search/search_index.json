{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Software Defined Infrastructure This documentation contains the solutions for Software Defined Infrastructure. Use the sidebar on the left to navigate the exercises. The solutions have been created by: Sascha Novak (43772, sn062@hdm-stuttgart.de) Dominik Walz (44169, dw084@hdm-stuttgart.de)","title":"Home"},{"location":"#software-defined-infrastructure","text":"This documentation contains the solutions for Software Defined Infrastructure. Use the sidebar on the left to navigate the exercises. The solutions have been created by: Sascha Novak (43772, sn062@hdm-stuttgart.de) Dominik Walz (44169, dw084@hdm-stuttgart.de)","title":"Software Defined Infrastructure"},{"location":"exercise01/","text":"Exercise 1: Server Creation Documentation A Debian 12 server with the name exercise-01 and a public IPv4 is created using the Hetzner web console. The SSH key of the local computer is attached to the server in order to avoid having to receive the password of the root account via mail. The created server has the IP 46.62.163.84 . After the login using ssh root@46.62.163.84 the following message appears: The authenticity of host '46.62.163.84 (46.62.163.84)' can't be established. ED25519 key fingerprint is SHA256:D2uCM+xEb8b/WQfiquGHiQWjh9xz3ZZFYBaoXtjDD9g. This key is not known by any other names. Are you sure you want to continue connecting (yes/no/[fingerprint])? Explanation When connecting to any machine using SSH a challenge-response protocol is being used to authenticate the user and the server against each other. First the user (in this case the local computer) provides its identity to the server, after that the server provides its identity to the client (the local computer). If the identity is not previously known it has to be manually accepted. If idenities are listed inside the known_hosts file, they are known and do not have to be specifically accepted. After manually accepting the new server identity, it can now be found in the known_hosts file of the local machine (for better readability HashKnownHosts has been set to no in the ~/.ssh/config file so that the hostnames are visible as cleartext): $ cat ~/.ssh/known_hosts 46.62.163.84 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5... 46.62.163.84 ssh-rsa AAAAB3NzaC1yc2EAAAAD... 46.62.163.84 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTI... Therefor any subsequent ssh connection to the server does not request the acceptance of the fingerprint of the server as long as the hostname of the server does not change. Additional Remarks The server does have to accept the identity of the client in the same way the client does - the identity must be authorized by the server on connection attempt. Since the server was created with an already provided SSH key (which is the public key of the local computer) it has already been appended to the authorized_keys file of the root account which in turn makes any client, which can prove this identity, authorized to connect as root to the system: root@exercise-01:~# cat ~/.ssh/authorized_keys ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDJzIgYTZw/zIxjxBqv2yJzB5buLEQgX6RKEowEOA4qL","title":"Exercise 1"},{"location":"exercise01/#exercise-1-server-creation","text":"","title":"Exercise 1: Server Creation"},{"location":"exercise01/#documentation","text":"A Debian 12 server with the name exercise-01 and a public IPv4 is created using the Hetzner web console. The SSH key of the local computer is attached to the server in order to avoid having to receive the password of the root account via mail. The created server has the IP 46.62.163.84 . After the login using ssh root@46.62.163.84 the following message appears: The authenticity of host '46.62.163.84 (46.62.163.84)' can't be established. ED25519 key fingerprint is SHA256:D2uCM+xEb8b/WQfiquGHiQWjh9xz3ZZFYBaoXtjDD9g. This key is not known by any other names. Are you sure you want to continue connecting (yes/no/[fingerprint])?","title":"Documentation"},{"location":"exercise01/#explanation","text":"When connecting to any machine using SSH a challenge-response protocol is being used to authenticate the user and the server against each other. First the user (in this case the local computer) provides its identity to the server, after that the server provides its identity to the client (the local computer). If the identity is not previously known it has to be manually accepted. If idenities are listed inside the known_hosts file, they are known and do not have to be specifically accepted. After manually accepting the new server identity, it can now be found in the known_hosts file of the local machine (for better readability HashKnownHosts has been set to no in the ~/.ssh/config file so that the hostnames are visible as cleartext): $ cat ~/.ssh/known_hosts 46.62.163.84 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5... 46.62.163.84 ssh-rsa AAAAB3NzaC1yc2EAAAAD... 46.62.163.84 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTI... Therefor any subsequent ssh connection to the server does not request the acceptance of the fingerprint of the server as long as the hostname of the server does not change.","title":"Explanation"},{"location":"exercise01/#additional-remarks","text":"The server does have to accept the identity of the client in the same way the client does - the identity must be authorized by the server on connection attempt. Since the server was created with an already provided SSH key (which is the public key of the local computer) it has already been appended to the authorized_keys file of the root account which in turn makes any client, which can prove this identity, authorized to connect as root to the system: root@exercise-01:~# cat ~/.ssh/authorized_keys ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDJzIgYTZw/zIxjxBqv2yJzB5buLEQgX6RKEowEOA4qL","title":"Additional Remarks"},{"location":"exercise02/","text":"Exercise 2: Server re-creation Documentation The server exercise-02 with the same configuration as the one from Exercise 1 has been created. Additionally the same IP addresses have been reused. When connecting to the server now using ssh root@46.62.163.84 the following error message appears: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ED25519 key sent by the remote host is SHA256:aPHbFitv2gzPSk6lqJVrNPKVjthnaJL270H0zY1QNt0. Please contact your system administrator. Add correct host key in /home/dwalz/.ssh/known_hosts to get rid of this message. Offending ECDSA key in /home/dwalz/.ssh/known_hosts:3 remove with: ssh-keygen -f '/home/dwalz/.ssh/known_hosts' -R '46.62.163.84' Host key for 46.62.163.84 has changed and you have requested strict checking. Host key verification failed. Explanation This message is happens because the identity associated with the host 46.62.163.84 inside the known_hosts file differs from the key the server has provided now. As soon as a known host provides a different identity, this error message occurs and warns the user from possible security implications . There is usually two reasons for such a scenario, both of which are listed in the error message: The first possibility is that the host changed its identity for some reason by exchanging the SSH key pair for a different one (for example if the private key of the server gets compromised). The second possibility and the possibility experienced now is that a new server is using the same hostname. In this specific case it is not security relevant since there is no relevant data on the created servers but the new server that is serving on the same host could also be a man in the middle server that is pretending to be the old service. In such a case it could read all the data that would usually be transmitted including sensitive information that ssh is built to handle. This is why it is very important to mistrust servers should such messages appear when connecting and double check if possible. Solution Since it is known that the change in this case stems from a new server being created replacing the old one at the same IP this warning can be disregarded as a normal occurence. To fix the error message the offending entries in the known_hosts file have to be removed either manually or using the command provided by the error message: ssh-keygen -f '~/.ssh/known_hosts' -R '46.62.163.84' After that the host 46.62.163.84 is no longer associated with the identity of the old server and the connection to the new server can be established normally.","title":"Exercise 2"},{"location":"exercise02/#exercise-2-server-re-creation","text":"","title":"Exercise 2: Server re-creation"},{"location":"exercise02/#documentation","text":"The server exercise-02 with the same configuration as the one from Exercise 1 has been created. Additionally the same IP addresses have been reused. When connecting to the server now using ssh root@46.62.163.84 the following error message appears: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ED25519 key sent by the remote host is SHA256:aPHbFitv2gzPSk6lqJVrNPKVjthnaJL270H0zY1QNt0. Please contact your system administrator. Add correct host key in /home/dwalz/.ssh/known_hosts to get rid of this message. Offending ECDSA key in /home/dwalz/.ssh/known_hosts:3 remove with: ssh-keygen -f '/home/dwalz/.ssh/known_hosts' -R '46.62.163.84' Host key for 46.62.163.84 has changed and you have requested strict checking. Host key verification failed.","title":"Documentation"},{"location":"exercise02/#explanation","text":"This message is happens because the identity associated with the host 46.62.163.84 inside the known_hosts file differs from the key the server has provided now. As soon as a known host provides a different identity, this error message occurs and warns the user from possible security implications . There is usually two reasons for such a scenario, both of which are listed in the error message: The first possibility is that the host changed its identity for some reason by exchanging the SSH key pair for a different one (for example if the private key of the server gets compromised). The second possibility and the possibility experienced now is that a new server is using the same hostname. In this specific case it is not security relevant since there is no relevant data on the created servers but the new server that is serving on the same host could also be a man in the middle server that is pretending to be the old service. In such a case it could read all the data that would usually be transmitted including sensitive information that ssh is built to handle. This is why it is very important to mistrust servers should such messages appear when connecting and double check if possible.","title":"Explanation"},{"location":"exercise02/#solution","text":"Since it is known that the change in this case stems from a new server being created replacing the old one at the same IP this warning can be disregarded as a normal occurence. To fix the error message the offending entries in the known_hosts file have to be removed either manually or using the command provided by the error message: ssh-keygen -f '~/.ssh/known_hosts' -R '46.62.163.84' After that the host 46.62.163.84 is no longer associated with the identity of the old server and the connection to the new server can be established normally.","title":"Solution"},{"location":"exercise03/","text":"Exercise 3: Improve your server\u2019s security A firewall with the name fw-exercise-03 has been created with a single inbound rule for ICMP traffic. The server exercise-03 has been created using the new firewall and the same procedure as the previous exercises. The server IP for exercise-03 is 46.62.163.84 . Pinging the server with ping 46.62.163.84 results in successful ping answers but trying to connect using ssh root@46.62.163.84 results in a timeout. Firewalls and SSH Firewalls work on a whitelist principle. This means that as soon as a firewall is applied all traffic is forbidden except the traffic allowed by the firewall\u2019s rules. Currently the firewall limits incoming traffic to use the ICMP protocol which is used to exchange operation information (for example the \u201calive-ness\u201d when using ping ) which results in traffic using the TCP protocol (and any other protocol) to be blocked. SSH is using TCP on port 22 and is therefor blocked. After adding an inbound TCP rule on port 22 both ping and ssh access are successful. Nginx Installation The server was updated and rebooted. After the reboot nginx has been installed and the status of the nginx service has been confirmed: # systemctl status nginx \u25cf nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; preset: enabled) Active: active (running) since Mon 2025-07-21 12:03:09 UTC; 10s ago ... Nginx is a server application that is used to host websites and other web services. The websites served by nginx are by default accessed using the HTTP protocol which is served via TCP on port 80 . The command wget can be used to access the content of the default generated website served by nginx : root@exercise-03:~# wget -O - http://46.62.163.84 Connecting to 46.62.163.84:80... connected. HTTP request sent, awaiting response... 200 OK Length: 615 [text/html] Saving to: \u2018STDOUT\u2019 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... Nginx & Firewall When trying to get the file from the local computer the connection fails with a timeout similarly to the SSH case before. The firewall once again blocks all incoming traffic that is not whitelisted by a firewall rule. Since HTTP used TCP over port 80 the current firewall setup blocks the incoming HTTP traffic. Since the firewall sits between the server and the internet, requests can be made from the server to itself. The firewall would block such traffic were it to receive it but since it is just routed using the loopback interface of the server it will not reach any other network interface other than the server itself. After an inbound rule for TCP port 80 is added the website can be accessed from outside too. $ wget -O - http://46.62.163.84 Connecting to 46.62.163.84:80... connected. HTTP request sent, awaiting response... 200 OK Length: 615 [text/html] Saving to: \u2018STDOUT\u2019 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ...","title":"Exercise 3"},{"location":"exercise03/#exercise-3-improve-your-servers-security","text":"A firewall with the name fw-exercise-03 has been created with a single inbound rule for ICMP traffic. The server exercise-03 has been created using the new firewall and the same procedure as the previous exercises. The server IP for exercise-03 is 46.62.163.84 . Pinging the server with ping 46.62.163.84 results in successful ping answers but trying to connect using ssh root@46.62.163.84 results in a timeout.","title":"Exercise 3: Improve your server\u2019s security"},{"location":"exercise03/#firewalls-and-ssh","text":"Firewalls work on a whitelist principle. This means that as soon as a firewall is applied all traffic is forbidden except the traffic allowed by the firewall\u2019s rules. Currently the firewall limits incoming traffic to use the ICMP protocol which is used to exchange operation information (for example the \u201calive-ness\u201d when using ping ) which results in traffic using the TCP protocol (and any other protocol) to be blocked. SSH is using TCP on port 22 and is therefor blocked. After adding an inbound TCP rule on port 22 both ping and ssh access are successful.","title":"Firewalls and SSH"},{"location":"exercise03/#nginx-installation","text":"The server was updated and rebooted. After the reboot nginx has been installed and the status of the nginx service has been confirmed: # systemctl status nginx \u25cf nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; preset: enabled) Active: active (running) since Mon 2025-07-21 12:03:09 UTC; 10s ago ... Nginx is a server application that is used to host websites and other web services. The websites served by nginx are by default accessed using the HTTP protocol which is served via TCP on port 80 . The command wget can be used to access the content of the default generated website served by nginx : root@exercise-03:~# wget -O - http://46.62.163.84 Connecting to 46.62.163.84:80... connected. HTTP request sent, awaiting response... 200 OK Length: 615 [text/html] Saving to: \u2018STDOUT\u2019 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ...","title":"Nginx Installation"},{"location":"exercise03/#nginx-firewall","text":"When trying to get the file from the local computer the connection fails with a timeout similarly to the SSH case before. The firewall once again blocks all incoming traffic that is not whitelisted by a firewall rule. Since HTTP used TCP over port 80 the current firewall setup blocks the incoming HTTP traffic. Since the firewall sits between the server and the internet, requests can be made from the server to itself. The firewall would block such traffic were it to receive it but since it is just routed using the loopback interface of the server it will not reach any other network interface other than the server itself. After an inbound rule for TCP port 80 is added the website can be accessed from outside too. $ wget -O - http://46.62.163.84 Connecting to 46.62.163.84:80... connected. HTTP request sent, awaiting response... 200 OK Length: 615 [text/html] Saving to: \u2018STDOUT\u2019 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ...","title":"Nginx &amp; Firewall"},{"location":"exercise04/","text":"Exercise 4: ssh-agent installation ssh-agent has been installed on the local system. Now every first time a login session needs to use the private key to establish connections the agent asks for the passphrase of the active key. Every following request is then answered by the ssh-agent alleviating the user of the need to enter the passphrase every time it has to be used by the SSH client. This can also be combined with the keychain package to further reduce the need to enter the passphrase to once every system reboot.","title":"Exercise 4"},{"location":"exercise04/#exercise-4-ssh-agent-installation","text":"ssh-agent has been installed on the local system. Now every first time a login session needs to use the private key to establish connections the agent asks for the passphrase of the active key. Every following request is then answered by the ssh-agent alleviating the user of the need to enter the passphrase every time it has to be used by the SSH client. This can also be combined with the keychain package to further reduce the need to enter the passphrase to once every system reboot.","title":"Exercise 4: ssh-agent installation"},{"location":"exercise05/","text":"Exercise 5: MI Gitlab access by ssh Under Preferences/SSH Keys the public SSH key of the local machine has been added. This has for example been used to push the first three exercises of this documentation to the MI Gitlab: # git remote add origin git@gitlab.mi.hdm-stuttgart.de:sdi-dw084/sdi.git # git push --set-upstream origin --all Enumerating objects: 6, done. Counting objects: 100% (6/6), done. Delta compression using up to 24 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (6/6), 3.96 KiB | 3.96 MiB/s, done. Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)","title":"Exercise 5"},{"location":"exercise05/#exercise-5-mi-gitlab-access-by-ssh","text":"Under Preferences/SSH Keys the public SSH key of the local machine has been added. This has for example been used to push the first three exercises of this documentation to the MI Gitlab: # git remote add origin git@gitlab.mi.hdm-stuttgart.de:sdi-dw084/sdi.git # git push --set-upstream origin --all Enumerating objects: 6, done. Counting objects: 100% (6/6), done. Delta compression using up to 24 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (6/6), 3.96 KiB | 3.96 MiB/s, done. Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)","title":"Exercise 5: MI Gitlab access by ssh"},{"location":"exercise06/","text":"Exercise 6: ssh host hopping Two Debian 12 servers named host-a and host-b were created via the Hetzner Cloud console. Both servers were configured with the same public SSH key to allow access from the local workstation. host-a has both the public IPv4 65.21.182.46 and is part of a private network with the IP 10.0.0.2 . host-b has only a private IPv4 and is also part of the same private network with the IP 10.0.0.3 . On the local workstation, the private SSH key is loaded into the SSH agent using: ssh-add ~/.ssh/id_ed25519 Then with the agent forwarding active a SSH connection is established: ssh -A root@65.21.182.46 From host-a a second SSH connection is started to host-b using its private IP: ssh root@10.0.0.3 After logging out of both hosts, the connection sequence is repeated in the same order, first host-a then host-b . While logged inot host-b , an attempt is made to open a SSH connecetion into host-a. This results in a password prompt, despite agent forwarding being enabled on the inital workstation connection. Exit from host-b to then restart the connection from host-a to host-b this time using agent forwarding a second time: ssh -A root@10.0.0.2 Now when logged into host-b and conneceting back to host-a , no password prompt appears and the login is successfull. Explanation SSH agent forwarding allows the credentials of a local workstation to be securly forwarded to remote machines. This enables indirect authentication which means a user on host-a can prove his identity to host-b using the agent running on the original workstation without storing keys on any server. However agent forwarding is not transitive by default. If host-a connects to host-b without forwaring its own agent ( -A ), then host-b has no access to the forwaded credentials and cannot connect back to host-a without prompting for a password.","title":"Exercise 6"},{"location":"exercise06/#exercise-6-ssh-host-hopping","text":"Two Debian 12 servers named host-a and host-b were created via the Hetzner Cloud console. Both servers were configured with the same public SSH key to allow access from the local workstation. host-a has both the public IPv4 65.21.182.46 and is part of a private network with the IP 10.0.0.2 . host-b has only a private IPv4 and is also part of the same private network with the IP 10.0.0.3 . On the local workstation, the private SSH key is loaded into the SSH agent using: ssh-add ~/.ssh/id_ed25519 Then with the agent forwarding active a SSH connection is established: ssh -A root@65.21.182.46 From host-a a second SSH connection is started to host-b using its private IP: ssh root@10.0.0.3 After logging out of both hosts, the connection sequence is repeated in the same order, first host-a then host-b . While logged inot host-b , an attempt is made to open a SSH connecetion into host-a. This results in a password prompt, despite agent forwarding being enabled on the inital workstation connection. Exit from host-b to then restart the connection from host-a to host-b this time using agent forwarding a second time: ssh -A root@10.0.0.2 Now when logged into host-b and conneceting back to host-a , no password prompt appears and the login is successfull.","title":"Exercise 6: ssh host hopping"},{"location":"exercise06/#explanation","text":"SSH agent forwarding allows the credentials of a local workstation to be securly forwarded to remote machines. This enables indirect authentication which means a user on host-a can prove his identity to host-b using the agent running on the original workstation without storing keys on any server. However agent forwarding is not transitive by default. If host-a connects to host-b without forwaring its own agent ( -A ), then host-b has no access to the forwaded credentials and cannot connect back to host-a without prompting for a password.","title":"Explanation"},{"location":"exercise07/","text":"Exercise 7: ssh port forwarding The server exercise-07 was created with the same configurations as the final one from Exercise 3 . The server has the same IP as host-a from Exercise 6 . A quick test shows that Nginx is accessible: curl http://95.216.223.223 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... The firewall configuration was updated to allow only SSH (port 22 ) and block HTTP (port 80 ) from public access. Attempting to access the HTTP service from the local machine again shows: curl -O - http://65.21.182.46 curl: (28) Failed to connect to 65.21.182.46 port 80 after 75003 ms: Couldn't connect to server To test HTTP access securly a local SSH tunnel is needed: ssh -L 2000:localhost:80 root@65.21.182.46 -N This command forwards local port 2000 on the workstation to port 80 on the remote server through SSH. Visiting http://localhost:2000 in a local brwoser renders the default Nginx welcome page, confirming that the HTTP service is accessible only through the SSH tunnel. Explanation SSH local port forwarding is a powerful feature that allows tunneling of TCP connecetions through a secure channel. By forwarding local port 2000 to port 80 on the remote host, it becomes possible to access the remote web server without exposing it to the public internet. This approch is useful for secure access to internal or restricted servies without altering firewall rules.","title":"Exercise 7"},{"location":"exercise07/#exercise-7-ssh-port-forwarding","text":"The server exercise-07 was created with the same configurations as the final one from Exercise 3 . The server has the same IP as host-a from Exercise 6 . A quick test shows that Nginx is accessible: curl http://95.216.223.223 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... The firewall configuration was updated to allow only SSH (port 22 ) and block HTTP (port 80 ) from public access. Attempting to access the HTTP service from the local machine again shows: curl -O - http://65.21.182.46 curl: (28) Failed to connect to 65.21.182.46 port 80 after 75003 ms: Couldn't connect to server To test HTTP access securly a local SSH tunnel is needed: ssh -L 2000:localhost:80 root@65.21.182.46 -N This command forwards local port 2000 on the workstation to port 80 on the remote server through SSH. Visiting http://localhost:2000 in a local brwoser renders the default Nginx welcome page, confirming that the HTTP service is accessible only through the SSH tunnel.","title":"Exercise 7: ssh port forwarding"},{"location":"exercise07/#explanation","text":"SSH local port forwarding is a powerful feature that allows tunneling of TCP connecetions through a secure channel. By forwarding local port 2000 to port 80 on the remote host, it becomes possible to access the remote web server without exposing it to the public internet. This approch is useful for secure access to internal or restricted servies without altering firewall rules.","title":"Explanation"},{"location":"exercise08/","text":"Exercise 8: ssh X11 forwarding The server exercise-08 was created with the same configurations as the final one from Exercise 3 . To enable remote graphical access the following packages were needed: apt install -y firefox xauth After the installation, the SSH connection was closed. Then server was accessed again from the local workstation using X11 forwarding : ssh -Y 157.180.78.16 Once logged in Firefox browser was started on the remote machine using: firefox & The Firefox window appeared locally on the workstation desktop. Within Firefox navigating to: http://localhost This will show the welcome page of Nginx confirming that the browser running on the server could access the local Nginx service. Explanation X11 forwarding allows GUI applications from a remote Linux system shown displayed on a local machine. The -Y flag tells SSH to securely forward X11 traffic, enabling programs like Firefox to run remotely but render locally. Note for macOS and Windows: X11 support requires an external X server (XQuartz on macOS or VcXsrv/Xming on Windows). This setup allows secure remote GUI access without exposing services like Nginx to the public.","title":"Exercise 8"},{"location":"exercise08/#exercise-8-ssh-x11-forwarding","text":"The server exercise-08 was created with the same configurations as the final one from Exercise 3 . To enable remote graphical access the following packages were needed: apt install -y firefox xauth After the installation, the SSH connection was closed. Then server was accessed again from the local workstation using X11 forwarding : ssh -Y 157.180.78.16 Once logged in Firefox browser was started on the remote machine using: firefox & The Firefox window appeared locally on the workstation desktop. Within Firefox navigating to: http://localhost This will show the welcome page of Nginx confirming that the browser running on the server could access the local Nginx service.","title":"Exercise 8: ssh X11 forwarding"},{"location":"exercise08/#explanation","text":"X11 forwarding allows GUI applications from a remote Linux system shown displayed on a local machine. The -Y flag tells SSH to securely forward X11 traffic, enabling programs like Firefox to run remotely but render locally. Note for macOS and Windows: X11 support requires an external X server (XQuartz on macOS or VcXsrv/Xming on Windows). This setup allows secure remote GUI access without exposing services like Nginx to the public.","title":"Explanation"},{"location":"exercise09/","text":"Exercise 9: Enabling index based file search Using the same server as Exercise 8 with the following IP 157.180.78.16. The plocate package was installed to provide a file lookup mechanism using prebuilt indexes: apt install -y plocate To use the locate application first a system-wide index of all files was created using: sudo updatedb To test the functionality, a search was done for known system files: root@exercise-09:~# locate aptitude /etc/alternatives/aptitude /etc/alternatives/aptitude.8.gz /etc/alternatives/aptitude.cs.8.gz ... Then a new file was created in the system using: touch /root/mylocaltest.txt Attempting to locate the new file immediately: locate mylocaltest However this returned no results , as the file was not yet included in the index. So we have to rebuild the index again: sudo updatedb After building the index new the file could now to be found via locate . root@exercise-09:~# locate mylocaltest /root/mylocaltest.txt Next the file was deleted: rm /root/mylocaltest.txt A new locate call still returned the file path, since the index had not yet been updated. Only after rebuilding the index did the file disappear from the locate results: sudo updatedb Explanation plocate relies on a periodically updated database of the file system. It does not track real-time changes , meaning newly created or deleted files are not reflected until updatedb is rerun. So the best way to use this would be to run scripts in fixed intervals, like on startup or before shutting down the system. It is also possible to add a cronjob for the updating on the index in which case the updatedb command would automatically be called once in a while to keep the search index from becoming too out of date.","title":"Exercise 9"},{"location":"exercise09/#exercise-9-enabling-index-based-file-search","text":"Using the same server as Exercise 8 with the following IP 157.180.78.16. The plocate package was installed to provide a file lookup mechanism using prebuilt indexes: apt install -y plocate To use the locate application first a system-wide index of all files was created using: sudo updatedb To test the functionality, a search was done for known system files: root@exercise-09:~# locate aptitude /etc/alternatives/aptitude /etc/alternatives/aptitude.8.gz /etc/alternatives/aptitude.cs.8.gz ... Then a new file was created in the system using: touch /root/mylocaltest.txt Attempting to locate the new file immediately: locate mylocaltest However this returned no results , as the file was not yet included in the index. So we have to rebuild the index again: sudo updatedb After building the index new the file could now to be found via locate . root@exercise-09:~# locate mylocaltest /root/mylocaltest.txt Next the file was deleted: rm /root/mylocaltest.txt A new locate call still returned the file path, since the index had not yet been updated. Only after rebuilding the index did the file disappear from the locate results: sudo updatedb","title":"Exercise 9: Enabling index based file search"},{"location":"exercise09/#explanation","text":"plocate relies on a periodically updated database of the file system. It does not track real-time changes , meaning newly created or deleted files are not reflected until updatedb is rerun. So the best way to use this would be to run scripts in fixed intervals, like on startup or before shutting down the system. It is also possible to add a cronjob for the updating on the index in which case the updatedb command would automatically be called once in a while to keep the search index from becoming too out of date.","title":"Explanation"},{"location":"exercise10/","text":"Exercise 10: Using the tail -f command Using the same server as Exercise 8 with the following IP 157.180.78.16. On Debian 12 the /var/log/auth.log file is not available by default because they use journald for system logging. To monitor SSH login events in real-time, the following command was used: sudo journalctl -f -u ssh This shows a live log of the entries related specifically to the SSH service ( -u ssh ). A separate terminal window was used to cennect to the server via SSH. As expected new entries appeared immediately in the journal output, loggin the new session. After a few minutes of observation new login attempts were recorded without us inititating a new connection. These attempts were clearly unrelated to any of our active sessions and are likely automated brute-force attacks from external sources. Example log output: Jul 28 15:21:42 indexsearch sshd[2625]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=36.67.70.198 user=root Jul 28 15:21:43 indexsearch sshd[2623]: Received disconnect from 157.180.78.16 port 59158:11: Bye Bye [preauth] Jul 28 15:21:43 indexsearch sshd[2623]: Disconnected from authenticating user root 157.180.78.16 port 59158 [preauth] Jul 28 15:21:44 indexsearch sshd[2625]: Failed password for root from 157.180.78.16 port 49436 ssh2 Jul 28 15:21:45 indexsearch sshd[2625]: Received disconnect from 157.180.78.16 port 49436:11: Bye Bye [preauth] Jul 28 15:21:45 indexsearch sshd[2625]: Disconnected from authenticating user root 157.180.78.16 port 49436 [preauth] Explanation The journalctl command provides access to all system logs managed by systemd . By filtering with -u ssh , only logs from the SSH daemon ( sshd ) are displayed, which is ideal for monitoring login events. The observed log entries indicate repeated failed login attempts from unknown IPs, which are likely the result of automated scans or brute-force attacks on port 22. This is a common occurrence for publicly accessible servers and shows the importance of: Disabling password authentication Using SSH keys only Considering tools like fail2ban or firewall rate-limiting to reduce attack surface","title":"Exercise 10"},{"location":"exercise10/#exercise-10-using-the-tail-f-command","text":"Using the same server as Exercise 8 with the following IP 157.180.78.16. On Debian 12 the /var/log/auth.log file is not available by default because they use journald for system logging. To monitor SSH login events in real-time, the following command was used: sudo journalctl -f -u ssh This shows a live log of the entries related specifically to the SSH service ( -u ssh ). A separate terminal window was used to cennect to the server via SSH. As expected new entries appeared immediately in the journal output, loggin the new session. After a few minutes of observation new login attempts were recorded without us inititating a new connection. These attempts were clearly unrelated to any of our active sessions and are likely automated brute-force attacks from external sources. Example log output: Jul 28 15:21:42 indexsearch sshd[2625]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=36.67.70.198 user=root Jul 28 15:21:43 indexsearch sshd[2623]: Received disconnect from 157.180.78.16 port 59158:11: Bye Bye [preauth] Jul 28 15:21:43 indexsearch sshd[2623]: Disconnected from authenticating user root 157.180.78.16 port 59158 [preauth] Jul 28 15:21:44 indexsearch sshd[2625]: Failed password for root from 157.180.78.16 port 49436 ssh2 Jul 28 15:21:45 indexsearch sshd[2625]: Received disconnect from 157.180.78.16 port 49436:11: Bye Bye [preauth] Jul 28 15:21:45 indexsearch sshd[2625]: Disconnected from authenticating user root 157.180.78.16 port 49436 [preauth]","title":"Exercise 10: Using the tail -f command"},{"location":"exercise10/#explanation","text":"The journalctl command provides access to all system logs managed by systemd . By filtering with -u ssh , only logs from the SSH daemon ( sshd ) are displayed, which is ideal for monitoring login events. The observed log entries indicate repeated failed login attempts from unknown IPs, which are likely the result of automated scans or brute-force attacks on port 22. This is a common occurrence for publicly accessible servers and shows the importance of: Disabling password authentication Using SSH keys only Considering tools like fail2ban or firewall rate-limiting to reduce attack surface","title":"Explanation"},{"location":"exercise11/","text":"Exercise 11: Incrementally creating a base system Click here to view the solution in the repository. Minimal Configuration and Basics The minimal configuration contained in the mentioned figure has been copied into the file main.tf . Running terraform init initializes the provider module from the Hetzner Cloud specified at the top of the file. This allows us to use all the definitions of Hetzner in Terraform to manage the server infrastructure of the project via Terraform. The hcloud_server resource which is specifying a server instance in ouc configuration has been renamed to exercise_11 for better identification. Running terraform plan right now, Terraform will us inform of changes it will make to the current configuration present in the cloud. The proposed changes follow the specifications made in the configuration: $ terraform plan ... Terraform will perform the following actions: # hcloud_server.exercise_11 will be created + resource \"hcloud_server\" \"exercise_11\" { + image = \"debian-12\" + name = \"exercise-11\" + server_type = \"cx22\" + ipv4_address = (known after apply) ... } Plan: 1 to add, 0 to change, 0 to destroy. Adding the SSH Firewall To add an SSH firewall to the server a hcloud_firewall resource has to be created which represents a firewall object in the Hetzner Cloud. The firewall has to have a single inbound rule for TCP traffic on port 22: resource \"hcloud_firewall\" \"fw_ssh\" { name = \"exercise-11-fw-ssh\" rule { description = \"SSH inbound\" direction = \"in\" protocol = \"tcp\" port = 22 source_ips = [\"0.0.0.0/0\", \"::/0\"] } } Important to notice here is that the rule also has to specify the source_ips which is the list of allowed IPs that the SSH traffic may be sent from. Setting them to 0.0.0.0/0 for IPv4 and ::/0 for IPv6 addresses respectively allows any sender on the internet to send SSH packets to the server. To apply the firewall to the server, it has to be connected to it in the terraform configuration. The hcloud_server resource has the field firewall_ids which can be used to provide a list of IDs of firewalls to apply to that server. The firewall resource that has been created is passed into this list to apply the firewall to the server. Terraform allows us to reference the field of other resources by reference to fill them out once they become available. This means there is no need for manually transferring the ID of a created firewall resouce to the server resource definition: resource \"hcloud_server\" \"exercise_11\" { name = \"exercise-11\" image = \"debian-12\" server_type = \"cx22\" firewall_ids = [hcloud_firewall.fw_ssh.id] } Applying the Configuration to the Cloud Running terraform plan will once again show changes that will be made to the infrastructure. In this case there will be two resources created: The firewall and the server resource. With terraform apply the planned configuration will then be acted upon and will be created in the Hetzner Cloud. In the Cloud Console the created server and firewall can now be observed. The server has an IP of 37.27.219.19 and the applied firewall can be seen. Pinging the server now would fail , since there is no ICMP firewall configured for the server which would allow any ping traffic to it. The only way to reach it would be via SSH using ssh root@37.27.219.19 . Sadly, the automatic E-Mail was never sent to the E-Mail account provided to Hetzner and therefor the root password was unknown. It can be reset using the Rescue tab in the Cloud Console to still connect to the server. A way better option however is to provide a SSH key to the server upon creation like in Exercise 1 . Version Control and Secrets Currently the private API token for the Hetzner API is readable in plain text inside the main.tf file, making it unable to be used in version control like git . To circumvent the issue the token has to be specified as a variable which can then be used in the initialization of the hcloud provider. This makes it possible to load the variable from another file and then apply it without it ever having to be present in the configuration file: variable \"hcloud_api_token\" { description = \"API token for the Hetzner Cloud\" type = string sensitive = true } This variable can now be used instead of the hardcoded value inside the hcloud provider: provider \"hcloud\" { token = var.hcloud_api_token } Now the variable only has to be specified. Right now when using terraform apply Terraform will ask for the value of the variable: $ terraform apply var.hcloud_api_token API token for the Hetzner Cloud Enter a value: We can either input the API token every time using the CLI or alternatively use a *.tfvars file to specify the variables. In this case the file secrets.tfvars is used to provide a value to the hcloud_api_token : hcloud_api_token = \"...\" The variable file has to be loaded during the application using terraform apply -var-file=secrets.tfvars . To prevent the necessity of having to provide the file every time a Terraform command has to be used, the variable file can also be named *.auto.tfvars to load them automatically. This way the usage of Terraform remains simple. Now the *.tfvars files can be excluded from versioning to keep any sensitive information outside of version control. SSH Passwordless Login To be able to log in to the server without using a password we have to add the public key of the local machine to the server\u2019s authorized_keys file. To register a SSH key in the Hetzner Cloud the hcloud_ssh_key resource is used. In it the public_key can be provided from either a file or as a string: resource \"hcloud_ssh_key\" \"dw084_ssh_key\" { name = \"dw084-ssh-key\" public_key = file(\"~/.ssh/id_ed25519.pub\") } This SSH key can then be added to the server using the ssh_keys field in which a list of hcloud_ssh_key IDs can be specified to be added to the respective server: resource \"hcloud_server\" \"exercise_11\" { ... ssh_keys = [hcloud_ssh_key.dw084_ssh_key.id] } After using terraform apply to create the infrastructure the login into the server works without having to enter any passwords: $ ssh root@37.27.219.19 ... root@exercise-11:~# Creating Outputs for Server Properties Some properties of created resources like the server\u2019s IP can\u2019t be known before applying the configuration. After the creation these values will become known to Terraform and can be displayed using a file named outputs.tf . In this file there may be arbitrary output sections created. They all have a value which may reference any static or dynamic values to display after successful usage of the terraform apply command. To display the IP and datacenter of the created server the ipv4_address and datacenter attribute of the hcloud_server resource are used respectively: output \"server_ip\" { description = \"IP of the server created in exercise 11\" value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { description = \"Datacenter of the server created in exercise 11\" value = hcloud_server.exercise_11.datacenter } Now these two values are being displayed when running terraform apply : $ terraform apply ... server_datacenter = \"hel1-dc2\" server_ip = \"37.27.219.19\"","title":"Exercise 11"},{"location":"exercise11/#exercise-11-incrementally-creating-a-base-system","text":"Click here to view the solution in the repository.","title":"Exercise 11: Incrementally creating a base system"},{"location":"exercise11/#minimal-configuration-and-basics","text":"The minimal configuration contained in the mentioned figure has been copied into the file main.tf . Running terraform init initializes the provider module from the Hetzner Cloud specified at the top of the file. This allows us to use all the definitions of Hetzner in Terraform to manage the server infrastructure of the project via Terraform. The hcloud_server resource which is specifying a server instance in ouc configuration has been renamed to exercise_11 for better identification. Running terraform plan right now, Terraform will us inform of changes it will make to the current configuration present in the cloud. The proposed changes follow the specifications made in the configuration: $ terraform plan ... Terraform will perform the following actions: # hcloud_server.exercise_11 will be created + resource \"hcloud_server\" \"exercise_11\" { + image = \"debian-12\" + name = \"exercise-11\" + server_type = \"cx22\" + ipv4_address = (known after apply) ... } Plan: 1 to add, 0 to change, 0 to destroy.","title":"Minimal Configuration and Basics"},{"location":"exercise11/#adding-the-ssh-firewall","text":"To add an SSH firewall to the server a hcloud_firewall resource has to be created which represents a firewall object in the Hetzner Cloud. The firewall has to have a single inbound rule for TCP traffic on port 22: resource \"hcloud_firewall\" \"fw_ssh\" { name = \"exercise-11-fw-ssh\" rule { description = \"SSH inbound\" direction = \"in\" protocol = \"tcp\" port = 22 source_ips = [\"0.0.0.0/0\", \"::/0\"] } } Important to notice here is that the rule also has to specify the source_ips which is the list of allowed IPs that the SSH traffic may be sent from. Setting them to 0.0.0.0/0 for IPv4 and ::/0 for IPv6 addresses respectively allows any sender on the internet to send SSH packets to the server. To apply the firewall to the server, it has to be connected to it in the terraform configuration. The hcloud_server resource has the field firewall_ids which can be used to provide a list of IDs of firewalls to apply to that server. The firewall resource that has been created is passed into this list to apply the firewall to the server. Terraform allows us to reference the field of other resources by reference to fill them out once they become available. This means there is no need for manually transferring the ID of a created firewall resouce to the server resource definition: resource \"hcloud_server\" \"exercise_11\" { name = \"exercise-11\" image = \"debian-12\" server_type = \"cx22\" firewall_ids = [hcloud_firewall.fw_ssh.id] }","title":"Adding the SSH Firewall"},{"location":"exercise11/#applying-the-configuration-to-the-cloud","text":"Running terraform plan will once again show changes that will be made to the infrastructure. In this case there will be two resources created: The firewall and the server resource. With terraform apply the planned configuration will then be acted upon and will be created in the Hetzner Cloud. In the Cloud Console the created server and firewall can now be observed. The server has an IP of 37.27.219.19 and the applied firewall can be seen. Pinging the server now would fail , since there is no ICMP firewall configured for the server which would allow any ping traffic to it. The only way to reach it would be via SSH using ssh root@37.27.219.19 . Sadly, the automatic E-Mail was never sent to the E-Mail account provided to Hetzner and therefor the root password was unknown. It can be reset using the Rescue tab in the Cloud Console to still connect to the server. A way better option however is to provide a SSH key to the server upon creation like in Exercise 1 .","title":"Applying the Configuration to the Cloud"},{"location":"exercise11/#version-control-and-secrets","text":"Currently the private API token for the Hetzner API is readable in plain text inside the main.tf file, making it unable to be used in version control like git . To circumvent the issue the token has to be specified as a variable which can then be used in the initialization of the hcloud provider. This makes it possible to load the variable from another file and then apply it without it ever having to be present in the configuration file: variable \"hcloud_api_token\" { description = \"API token for the Hetzner Cloud\" type = string sensitive = true } This variable can now be used instead of the hardcoded value inside the hcloud provider: provider \"hcloud\" { token = var.hcloud_api_token } Now the variable only has to be specified. Right now when using terraform apply Terraform will ask for the value of the variable: $ terraform apply var.hcloud_api_token API token for the Hetzner Cloud Enter a value: We can either input the API token every time using the CLI or alternatively use a *.tfvars file to specify the variables. In this case the file secrets.tfvars is used to provide a value to the hcloud_api_token : hcloud_api_token = \"...\" The variable file has to be loaded during the application using terraform apply -var-file=secrets.tfvars . To prevent the necessity of having to provide the file every time a Terraform command has to be used, the variable file can also be named *.auto.tfvars to load them automatically. This way the usage of Terraform remains simple. Now the *.tfvars files can be excluded from versioning to keep any sensitive information outside of version control.","title":"Version Control and Secrets"},{"location":"exercise11/#ssh-passwordless-login","text":"To be able to log in to the server without using a password we have to add the public key of the local machine to the server\u2019s authorized_keys file. To register a SSH key in the Hetzner Cloud the hcloud_ssh_key resource is used. In it the public_key can be provided from either a file or as a string: resource \"hcloud_ssh_key\" \"dw084_ssh_key\" { name = \"dw084-ssh-key\" public_key = file(\"~/.ssh/id_ed25519.pub\") } This SSH key can then be added to the server using the ssh_keys field in which a list of hcloud_ssh_key IDs can be specified to be added to the respective server: resource \"hcloud_server\" \"exercise_11\" { ... ssh_keys = [hcloud_ssh_key.dw084_ssh_key.id] } After using terraform apply to create the infrastructure the login into the server works without having to enter any passwords: $ ssh root@37.27.219.19 ... root@exercise-11:~#","title":"SSH Passwordless Login"},{"location":"exercise11/#creating-outputs-for-server-properties","text":"Some properties of created resources like the server\u2019s IP can\u2019t be known before applying the configuration. After the creation these values will become known to Terraform and can be displayed using a file named outputs.tf . In this file there may be arbitrary output sections created. They all have a value which may reference any static or dynamic values to display after successful usage of the terraform apply command. To display the IP and datacenter of the created server the ipv4_address and datacenter attribute of the hcloud_server resource are used respectively: output \"server_ip\" { description = \"IP of the server created in exercise 11\" value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { description = \"Datacenter of the server created in exercise 11\" value = hcloud_server.exercise_11.datacenter } Now these two values are being displayed when running terraform apply : $ terraform apply ... server_datacenter = \"hel1-dc2\" server_ip = \"37.27.219.19\"","title":"Creating Outputs for Server Properties"},{"location":"exercise12/","text":"Exercise 12: Automatic Nginx installation Click here to view the solution in the repository. The following script can be used to install, start and enable (make it survive a re-boot) the nginx package on the server the following script has been created. It will update the system first apt-get update and apt-get -y upgrade and after that install Nginx and enable it using the systemctl : #!/bin/sh # Update installation apt-get update apt-get -y upgrade # Install nginx apt-get -y install nginx # Start and enable (survive after re-boot) nginx systemctl start nginx systemctl enable nginx To make the script run on server start the user_data argument is used to pass the file to terraform. It will be applied to the server once it starts. To be able to reach the webserver and test the success of installing nginx the server needs a firewall that allows incoming HTTP traffic. This is done by adding a rule for the TCP protocol on port 80: resource \"hcloud_firewall\" \"fw_exercise_12\" { name = \"exercise-12-fw\" ... rule { description = \"HTTP inbound\" direction = \"in\" protocol = \"tcp\" port = 80 source_ips = [\"0.0.0.0/0\", \"::/0\"] } } When accessing the website in the browser under http://95.216.223.223 the nginx default landing page can be observed. Even after a server restart there is no additional commands that need to be executed on the server to make nginx running again thus showing the landing page immediately.","title":"Exercise 12"},{"location":"exercise12/#exercise-12-automatic-nginx-installation","text":"Click here to view the solution in the repository. The following script can be used to install, start and enable (make it survive a re-boot) the nginx package on the server the following script has been created. It will update the system first apt-get update and apt-get -y upgrade and after that install Nginx and enable it using the systemctl : #!/bin/sh # Update installation apt-get update apt-get -y upgrade # Install nginx apt-get -y install nginx # Start and enable (survive after re-boot) nginx systemctl start nginx systemctl enable nginx To make the script run on server start the user_data argument is used to pass the file to terraform. It will be applied to the server once it starts. To be able to reach the webserver and test the success of installing nginx the server needs a firewall that allows incoming HTTP traffic. This is done by adding a rule for the TCP protocol on port 80: resource \"hcloud_firewall\" \"fw_exercise_12\" { name = \"exercise-12-fw\" ... rule { description = \"HTTP inbound\" direction = \"in\" protocol = \"tcp\" port = 80 source_ips = [\"0.0.0.0/0\", \"::/0\"] } } When accessing the website in the browser under http://95.216.223.223 the nginx default landing page can be observed. Even after a server restart there is no additional commands that need to be executed on the server to make nginx running again thus showing the landing page immediately.","title":"Exercise 12: Automatic Nginx installation"},{"location":"exercise13/","text":"Exercise 13: Working on Cloud-init Click here to view the solution in the repository. Now instead of using a shell script to install and initialize nginx , cloud-init is used. Similarly to how Terraform is providing the capabilities to describe an architecture as code and let terraform handle the setup process, cloud-init provides the capabilites to describe the server state as code and setup the sever respectively. The following cloud-init configuration is used to install and enable nginx as well as provide a custom landing page: #cloud-config packages: - nginx runcmd: - systemctl enable nginx - rm /var/www/html/* - > echo \"I'm Nginx @ $(dig -4 TXT +short o-o.myaddr.l.google.com @ns1.google.com) created $(date -u)\" >> /var/www/html/index.html The file can be passed as user_data into the server the same way that the nginx installation script has been before: resource \"hcloud_server\" \"exercise_13\" { ... user_data = file(\"cloud_init.yml\") } Securing SSH login When observing the journalctl log we can observe connection attempts over SSH to our machine. These are automated attacks from botnets and are meant to find and take over vulnerable servers that are connected to the internet. They try random username + password combinations in order attempt logging in to random machines on the internet. Within minutes of the start of the server multiple of those attempts have been observed: # journalctl -f ... Jul 28 15:23:34 exercise-13 sshd[1785]: Invalid user admin from 103.114.246.37 port 31632 Jul 28 15:23:34 exercise-13 sshd[1785]: pam_unix(sshd:auth): check pass; user unknown Jul 28 15:23:34 exercise-13 sshd[1785]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.114.246.37 Jul 28 15:23:36 exercise-13 sshd[1787]: error: kex_exchange_identification: Connection closed by remote host Jul 28 15:23:36 exercise-13 sshd[1787]: Connection closed by 196.251.114.29 port 51824 Jul 28 15:23:37 exercise-13 sshd[1785]: Failed password for invalid user admin from 103.114.246.37 port 31632 ssh2 Jul 28 15:23:37 exercise-13 sshd[1785]: Connection closed by invalid user admin 103.114.246.37 port 31632 [preauth] In order to mitigate these attacks disabling the password login via SSH altogether will prevent these attackers from randomly guessing the password. To disable the password login in cloud-init the option ssh_pwauth can be set to false . Additionally it is generally a bad idea to have a root user just lying about. It is better to have an alternative user for administrative purposes which can execute privileged commands using sudo . In cloud-init this can be achieved by first disabling the root user with the disable_root option and then create a new user in the users section with a name and a sudo option that allows it to execute using sudo . To be able to log in as the new user it needs to have an authorized SSH key that is accepted as passwordless login and can be specified using ssh-authorized-keys . The following config disables the password login and root user and sets up a new user named devops with root rights and bash as default shell: ssh_pwauth: false disable_root: true users: - name: devops sudo: [ALL=(ALL) NOPASSWD:ALL] lock_passwd: true ssh-authorized-keys: - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPZ4lA1SGICnXIgP1QUH8kLCzVFRQh3/hSlz+rBZtfUn shell: /bin/bash Attempting to login as root via ssh now fails. It doesn\u2019t fail with the message Permission denied for some reason, it rather says Please login as the user \"NONE\" rather than the user \"root\". which could be due to changes in cloud-init : $ ssh root@157.180.35.105 ... Please login as the user \"NONE\" rather than the user \"root\". The login to the newly created devops user is successful and the acquisition of root works too: $ ssh devops@157.180.35.105 ... devops@exercise-13:~$ sudo su - root@exercise-13:~# Investigating the /etc/ssh/sshd_config file shows, that root login has not been disabled fully with these options: $ grep PermitRoot /etc/ssh/sshd_config PermitRootLogin prohibit-password To fully disable root login this values has to be set to no which can be done using sed : ... runcmd: - sed -i -e '/^PermitRootLogin/s/^.*$/PermitRootLogin no/' /etc/ssh/sshd_config ... Installing fail2ban & plocate To install fail2ban and plocate both packages simply have to be added to the package list in the cloud-init file. But to make them both work an some additional work is required. In order for plocate to efficiently find files it needs to build a file index database. The plocate package comes with a updatedb command in order to do that. The command can simply be invoked in the runcmd section of the cloud-init file. For fail2ban there is a workaround necessary. The python3-systemd package has to be installed additionally to make the systemd backend available to fail2ban . On top of that the jail.local file has to be created with some parameters for the sshd configuration of fail2ban . Usually all default configuration lies in /etc/fail2ban/jail.conf . Additional configuration can just be added to the /etc/fail2ban/jail.local file which will then be merged with the existing configuration in jail.conf . The following section has to be added: [sshd] backend = systemd enabled = true This is done using the echo and tee commands in combination. After that the fail2ban service has to be restarted using sytemctl . All of the above steps are taken in the cloud-init file which now looks like the following: #cloud-config ssh_pwauth: false disable_root: true users: - name: devops sudo: [ALL=(ALL) NOPASSWD:ALL] lock_passwd: true ssh-authorized-keys: - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPZ4lA1SGICnXIgP1QUH8kLCzVFRQh3/hSlz+rBZtfUn shell: /bin/bash package_reboot_if_required: true package_update: true package_upgrade: true packages: - nginx - fail2ban - python3-systemd - plocate runcmd: - sed -i -e '/^PermitRootLogin/s/^.*$/PermitRootLogin no/' /etc/ssh/sshd_config # Fail2ban workaround - echo \"[sshd]\\nbackend = systemd\\nenabled = true\" | tee /etc/fail2ban/jail.local - systemctl restart fail2ban # Update plocate database - updatedb # Nginx - systemctl enable nginx - rm /var/www/html/* - > echo \"I'm Nginx @ $(dig -4 TXT +short o-o.myaddr.l.google.com @ns1.google.com) created $(date -u)\" >> /var/www/html/index.html With that the login, sudo commands, fail2ban and plocate work like intended: $ ssh devops@157.180.35.105 ... devops@exercise-13:~$ plocate ssh_host /etc/ssh/ssh_host_dsa_key /etc/ssh/ssh_host_dsa_key.pub ... devops@exercise-13:~$ sudo apt update ... All packages are up to date. devops@exercise-13:~$ sudo systemctl status fail2ban \u25cf fail2ban.service - Fail2Ban Service Loaded: loaded (/lib/systemd/system/fail2ban.service; enabled; preset: enabled) Active: active (running) since Mon 2025-07-28 16:36:22 UTC; 19min ago ... Firewall Attachment Issues At some point during the project Hetzner had issues with attaching, and especially removing firewalls from servers using the firewall_ids attribute in the server's definitions. There was a technical issue that made firewall detachment take way longer than expected, resulting in firewalls being still assigned to a server resource while the server itself was already destructed at that point. The firewalls couldn't be deleted anymore since they were still attached to a resource when in fact the resource was not existsing anymore leaving them orphaned. To stop this issue from occuring a firewall attachment has explicitly been specified to connect the firewall to the server. The Terraform lifecycle argument can then be used to force Terraform to replace the whole attachment on change of one of the resources referenced, stopping the implicit changes and attachments that were the issue before: resource \"hcloud_firewall_attachment\" \"exercise_fw_attachment\" { firewall_id = hcloud_firewall.fw_exercise_13.id server_ids = [hcloud_server.exercise_13.id] lifecycle { replace_triggered_by = [ hcloud_server.exercise_13, hcloud_firewall.fw_exercise_13 ] } } In the following exercises this method has been used throughout to mitigate this problem and prevent lots of orphaned firewalls in the group.","title":"Exercise 13"},{"location":"exercise13/#exercise-13-working-on-cloud-init","text":"Click here to view the solution in the repository. Now instead of using a shell script to install and initialize nginx , cloud-init is used. Similarly to how Terraform is providing the capabilities to describe an architecture as code and let terraform handle the setup process, cloud-init provides the capabilites to describe the server state as code and setup the sever respectively. The following cloud-init configuration is used to install and enable nginx as well as provide a custom landing page: #cloud-config packages: - nginx runcmd: - systemctl enable nginx - rm /var/www/html/* - > echo \"I'm Nginx @ $(dig -4 TXT +short o-o.myaddr.l.google.com @ns1.google.com) created $(date -u)\" >> /var/www/html/index.html The file can be passed as user_data into the server the same way that the nginx installation script has been before: resource \"hcloud_server\" \"exercise_13\" { ... user_data = file(\"cloud_init.yml\") }","title":"Exercise 13: Working on Cloud-init"},{"location":"exercise13/#securing-ssh-login","text":"When observing the journalctl log we can observe connection attempts over SSH to our machine. These are automated attacks from botnets and are meant to find and take over vulnerable servers that are connected to the internet. They try random username + password combinations in order attempt logging in to random machines on the internet. Within minutes of the start of the server multiple of those attempts have been observed: # journalctl -f ... Jul 28 15:23:34 exercise-13 sshd[1785]: Invalid user admin from 103.114.246.37 port 31632 Jul 28 15:23:34 exercise-13 sshd[1785]: pam_unix(sshd:auth): check pass; user unknown Jul 28 15:23:34 exercise-13 sshd[1785]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.114.246.37 Jul 28 15:23:36 exercise-13 sshd[1787]: error: kex_exchange_identification: Connection closed by remote host Jul 28 15:23:36 exercise-13 sshd[1787]: Connection closed by 196.251.114.29 port 51824 Jul 28 15:23:37 exercise-13 sshd[1785]: Failed password for invalid user admin from 103.114.246.37 port 31632 ssh2 Jul 28 15:23:37 exercise-13 sshd[1785]: Connection closed by invalid user admin 103.114.246.37 port 31632 [preauth] In order to mitigate these attacks disabling the password login via SSH altogether will prevent these attackers from randomly guessing the password. To disable the password login in cloud-init the option ssh_pwauth can be set to false . Additionally it is generally a bad idea to have a root user just lying about. It is better to have an alternative user for administrative purposes which can execute privileged commands using sudo . In cloud-init this can be achieved by first disabling the root user with the disable_root option and then create a new user in the users section with a name and a sudo option that allows it to execute using sudo . To be able to log in as the new user it needs to have an authorized SSH key that is accepted as passwordless login and can be specified using ssh-authorized-keys . The following config disables the password login and root user and sets up a new user named devops with root rights and bash as default shell: ssh_pwauth: false disable_root: true users: - name: devops sudo: [ALL=(ALL) NOPASSWD:ALL] lock_passwd: true ssh-authorized-keys: - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPZ4lA1SGICnXIgP1QUH8kLCzVFRQh3/hSlz+rBZtfUn shell: /bin/bash Attempting to login as root via ssh now fails. It doesn\u2019t fail with the message Permission denied for some reason, it rather says Please login as the user \"NONE\" rather than the user \"root\". which could be due to changes in cloud-init : $ ssh root@157.180.35.105 ... Please login as the user \"NONE\" rather than the user \"root\". The login to the newly created devops user is successful and the acquisition of root works too: $ ssh devops@157.180.35.105 ... devops@exercise-13:~$ sudo su - root@exercise-13:~# Investigating the /etc/ssh/sshd_config file shows, that root login has not been disabled fully with these options: $ grep PermitRoot /etc/ssh/sshd_config PermitRootLogin prohibit-password To fully disable root login this values has to be set to no which can be done using sed : ... runcmd: - sed -i -e '/^PermitRootLogin/s/^.*$/PermitRootLogin no/' /etc/ssh/sshd_config ...","title":"Securing SSH login"},{"location":"exercise13/#installing-fail2ban-plocate","text":"To install fail2ban and plocate both packages simply have to be added to the package list in the cloud-init file. But to make them both work an some additional work is required. In order for plocate to efficiently find files it needs to build a file index database. The plocate package comes with a updatedb command in order to do that. The command can simply be invoked in the runcmd section of the cloud-init file. For fail2ban there is a workaround necessary. The python3-systemd package has to be installed additionally to make the systemd backend available to fail2ban . On top of that the jail.local file has to be created with some parameters for the sshd configuration of fail2ban . Usually all default configuration lies in /etc/fail2ban/jail.conf . Additional configuration can just be added to the /etc/fail2ban/jail.local file which will then be merged with the existing configuration in jail.conf . The following section has to be added: [sshd] backend = systemd enabled = true This is done using the echo and tee commands in combination. After that the fail2ban service has to be restarted using sytemctl . All of the above steps are taken in the cloud-init file which now looks like the following: #cloud-config ssh_pwauth: false disable_root: true users: - name: devops sudo: [ALL=(ALL) NOPASSWD:ALL] lock_passwd: true ssh-authorized-keys: - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPZ4lA1SGICnXIgP1QUH8kLCzVFRQh3/hSlz+rBZtfUn shell: /bin/bash package_reboot_if_required: true package_update: true package_upgrade: true packages: - nginx - fail2ban - python3-systemd - plocate runcmd: - sed -i -e '/^PermitRootLogin/s/^.*$/PermitRootLogin no/' /etc/ssh/sshd_config # Fail2ban workaround - echo \"[sshd]\\nbackend = systemd\\nenabled = true\" | tee /etc/fail2ban/jail.local - systemctl restart fail2ban # Update plocate database - updatedb # Nginx - systemctl enable nginx - rm /var/www/html/* - > echo \"I'm Nginx @ $(dig -4 TXT +short o-o.myaddr.l.google.com @ns1.google.com) created $(date -u)\" >> /var/www/html/index.html With that the login, sudo commands, fail2ban and plocate work like intended: $ ssh devops@157.180.35.105 ... devops@exercise-13:~$ plocate ssh_host /etc/ssh/ssh_host_dsa_key /etc/ssh/ssh_host_dsa_key.pub ... devops@exercise-13:~$ sudo apt update ... All packages are up to date. devops@exercise-13:~$ sudo systemctl status fail2ban \u25cf fail2ban.service - Fail2Ban Service Loaded: loaded (/lib/systemd/system/fail2ban.service; enabled; preset: enabled) Active: active (running) since Mon 2025-07-28 16:36:22 UTC; 19min ago ...","title":"Installing fail2ban &amp; plocate"},{"location":"exercise13/#firewall-attachment-issues","text":"At some point during the project Hetzner had issues with attaching, and especially removing firewalls from servers using the firewall_ids attribute in the server's definitions. There was a technical issue that made firewall detachment take way longer than expected, resulting in firewalls being still assigned to a server resource while the server itself was already destructed at that point. The firewalls couldn't be deleted anymore since they were still attached to a resource when in fact the resource was not existsing anymore leaving them orphaned. To stop this issue from occuring a firewall attachment has explicitly been specified to connect the firewall to the server. The Terraform lifecycle argument can then be used to force Terraform to replace the whole attachment on change of one of the resources referenced, stopping the implicit changes and attachments that were the issue before: resource \"hcloud_firewall_attachment\" \"exercise_fw_attachment\" { firewall_id = hcloud_firewall.fw_exercise_13.id server_ids = [hcloud_server.exercise_13.id] lifecycle { replace_triggered_by = [ hcloud_server.exercise_13, hcloud_firewall.fw_exercise_13 ] } } In the following exercises this method has been used throughout to mitigate this problem and prevent lots of orphaned firewalls in the group.","title":"Firewall Attachment Issues"},{"location":"exercise14/","text":"Exercise 14: Solving the ~/.ssh/known_hosts quirk Click here to view the solution in the repository. As observed in Exercise 2 the ssh program warns the user every time the SSH fingerprint of a known host changes because such a change can be the indication of a man-in-the-middle attack. To prevent this error from occurring and to avoid the pollution of the ~/.ssh/known_hosts file on the local machine we can generate such a known_hosts file ourself and tell the ssh program (and related programs) to use this custom known_hosts file. Generation of the known_hosts file The generation of the known_hosts file requires us to know the fingerprint of the server. This is done by generating the SSH keypair on the local machine and transferring it to the server using cloud-init . The generation of a keypair can be done using the resource tls_private_key and specifying the signature algorithm used: resource \"tls_private_key\" \"server_ssh_key\" { algorithm = \"ED25519\" } This keypair will then be used when hydrating a cloud-init template file with values. The public and private key are filled into the file and the fully generated cloud-init file will then be run on the server. With the usage of a template file it is also possible to stub more than just the SSH keys. For simpler usage and transferability the previously hardcoded devops username and SSH public key of the local machine can also be injected dynamically allowing for more control of the resulting server setup. The following configuration is used in Terraform to create both the custom cloud_init as well as the known_hosts file: resource \"local_file\" \"cloud_init\" { filename = \"./gen/cloud_init.yml\" content = templatefile(\"./template/cloud_init.yml\", { default_user = var.server_username local_ssh_public = file(\"~/.ssh/id_ed25519.pub\") ssh_private = tls_private_key.server_ssh_key.private_key_openssh ssh_public = tls_private_key.server_ssh_key.public_key_openssh }) } resource \"local_file\" \"known_hosts\" { filename = \"./gen/known_hosts\" content = join(\" \", [hcloud_server.exercise_14.ipv4_address, tls_private_key.server_ssh_key.public_key_openssh]) file_permission = \"644\" } YAML Files and Indentation The templatefile function provided by Terraform simply replaces the template strings of the form ${var_name} with the provided values of the variable var_name . This can especially lead to issues when templating files in a format that is dependent on whitespace. Since the private key file of a SSH keypair is usually multiple lines long this leads to the follwing replacements inside the cloud-init file: ssh_keys: ed25519_public: ${ssh_public} ed25519_private: ${ssh_private} Will become: ssh_keys: ed25519_private: -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtz ... -----END OPENSSH PRIVATE KEY----- ed25519_public: ssh-ed25519 AAAC3NzaC1lZDI1NTE5AAAAIHu7IFxgFGxalBAzKagNefEWGxgoBf9Et+gpEjnEmLKC Which is not valid YAML. The cloud-init provision will fail and the server will not behave as specified. To resolve this issue there are two possible solutions: yamlencode . yamlencode is a function provided by Terraform that can encode a valid Terraform object structure into correctly formatted YAML: ${yamlencode({ ssh_keys = { ed25519_public = ssh_public ed25519_private = ssh_private } })} Will expand to: \"ssh_keys\": \"ed25519_private\": | -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtz ... -----END OPENSSH PRIVATE KEY----- \"ed25519_public\": | ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIHu7IFxgFGxalBAzKagNefEWGxgoBf9Et+gpEjnEmLKC Which is now correctly formatted. Generating Wrappers around ssh and scp To now use the custom known_hosts file we can specify it to the ssh program to use when connecting to the server: ssh -o UserKnownHostsFile=\"./gen/known_hosts\" devops@95.216.223.223 . To make connecting even more convenient we can now create a script that executes this code since both username and IP address are also known. Similarly the scp program which is used to transfer files between hosts using SSH can be wrapped in the same way. For both files a template was created and the templatefile function was used in a local_file resource for both files respectively: resource \"local_file\" \"ssh_bin\" { filename = \"./bin/ssh.sh\" content = templatefile(\"./template/ssh.sh\", { default_user = var.server_username ip = hcloud_server.exercise_14.ipv4_address }) file_permission = \"755\" depends_on = [local_file.known_hosts] } resource \"local_file\" \"scp_bin\" { filename = \"./bin/scp.sh\" content = templatefile(\"./template/scp.sh\", { default_user = var.server_username ip = hcloud_server.exercise_14.ipv4_address }) file_permission = \"755\" depends_on = [local_file.known_hosts] } Different to the other local_file resources (like known_hosts ) these need some additional arguments: The file permission 755 needs to be set which expands to rwxr-xr-x , giving the owner of the file read, write and execute rights while everyone else gets read and execute rights. This is important so that the generated script can be executed. Additionally the depends_on argument is set to mark this file dependent on the known_hosts file. It will only be generated after the known_hosts file has been generated and respectively will be destroyed before the known_hosts file will be destroyed to ensure the depenedency chain. The two files can now be used to transfer files between the local machine and the server and connect in a very convenient way, all without having to mess with any fingerprints again: $ ./bin/scp.sh ./main.tf devops@95.216.223.223:~/test.tf main.tf 100% 2625 65.5KB/s 00:00 $ ./bin/ssh.sh Linux exercise-14 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 ... devops@exercise-14:~$ ls test.tf devops@exercise-14:~$","title":"Exercise 14"},{"location":"exercise14/#exercise-14-solving-the-sshknown_hosts-quirk","text":"Click here to view the solution in the repository. As observed in Exercise 2 the ssh program warns the user every time the SSH fingerprint of a known host changes because such a change can be the indication of a man-in-the-middle attack. To prevent this error from occurring and to avoid the pollution of the ~/.ssh/known_hosts file on the local machine we can generate such a known_hosts file ourself and tell the ssh program (and related programs) to use this custom known_hosts file.","title":"Exercise 14: Solving the ~/.ssh/known_hosts quirk"},{"location":"exercise14/#generation-of-the-known_hosts-file","text":"The generation of the known_hosts file requires us to know the fingerprint of the server. This is done by generating the SSH keypair on the local machine and transferring it to the server using cloud-init . The generation of a keypair can be done using the resource tls_private_key and specifying the signature algorithm used: resource \"tls_private_key\" \"server_ssh_key\" { algorithm = \"ED25519\" } This keypair will then be used when hydrating a cloud-init template file with values. The public and private key are filled into the file and the fully generated cloud-init file will then be run on the server. With the usage of a template file it is also possible to stub more than just the SSH keys. For simpler usage and transferability the previously hardcoded devops username and SSH public key of the local machine can also be injected dynamically allowing for more control of the resulting server setup. The following configuration is used in Terraform to create both the custom cloud_init as well as the known_hosts file: resource \"local_file\" \"cloud_init\" { filename = \"./gen/cloud_init.yml\" content = templatefile(\"./template/cloud_init.yml\", { default_user = var.server_username local_ssh_public = file(\"~/.ssh/id_ed25519.pub\") ssh_private = tls_private_key.server_ssh_key.private_key_openssh ssh_public = tls_private_key.server_ssh_key.public_key_openssh }) } resource \"local_file\" \"known_hosts\" { filename = \"./gen/known_hosts\" content = join(\" \", [hcloud_server.exercise_14.ipv4_address, tls_private_key.server_ssh_key.public_key_openssh]) file_permission = \"644\" }","title":"Generation of the known_hosts file"},{"location":"exercise14/#yaml-files-and-indentation","text":"The templatefile function provided by Terraform simply replaces the template strings of the form ${var_name} with the provided values of the variable var_name . This can especially lead to issues when templating files in a format that is dependent on whitespace. Since the private key file of a SSH keypair is usually multiple lines long this leads to the follwing replacements inside the cloud-init file: ssh_keys: ed25519_public: ${ssh_public} ed25519_private: ${ssh_private} Will become: ssh_keys: ed25519_private: -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtz ... -----END OPENSSH PRIVATE KEY----- ed25519_public: ssh-ed25519 AAAC3NzaC1lZDI1NTE5AAAAIHu7IFxgFGxalBAzKagNefEWGxgoBf9Et+gpEjnEmLKC Which is not valid YAML. The cloud-init provision will fail and the server will not behave as specified. To resolve this issue there are two possible solutions: yamlencode . yamlencode is a function provided by Terraform that can encode a valid Terraform object structure into correctly formatted YAML: ${yamlencode({ ssh_keys = { ed25519_public = ssh_public ed25519_private = ssh_private } })} Will expand to: \"ssh_keys\": \"ed25519_private\": | -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtz ... -----END OPENSSH PRIVATE KEY----- \"ed25519_public\": | ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIHu7IFxgFGxalBAzKagNefEWGxgoBf9Et+gpEjnEmLKC Which is now correctly formatted.","title":"YAML Files and Indentation"},{"location":"exercise14/#generating-wrappers-around-ssh-and-scp","text":"To now use the custom known_hosts file we can specify it to the ssh program to use when connecting to the server: ssh -o UserKnownHostsFile=\"./gen/known_hosts\" devops@95.216.223.223 . To make connecting even more convenient we can now create a script that executes this code since both username and IP address are also known. Similarly the scp program which is used to transfer files between hosts using SSH can be wrapped in the same way. For both files a template was created and the templatefile function was used in a local_file resource for both files respectively: resource \"local_file\" \"ssh_bin\" { filename = \"./bin/ssh.sh\" content = templatefile(\"./template/ssh.sh\", { default_user = var.server_username ip = hcloud_server.exercise_14.ipv4_address }) file_permission = \"755\" depends_on = [local_file.known_hosts] } resource \"local_file\" \"scp_bin\" { filename = \"./bin/scp.sh\" content = templatefile(\"./template/scp.sh\", { default_user = var.server_username ip = hcloud_server.exercise_14.ipv4_address }) file_permission = \"755\" depends_on = [local_file.known_hosts] } Different to the other local_file resources (like known_hosts ) these need some additional arguments: The file permission 755 needs to be set which expands to rwxr-xr-x , giving the owner of the file read, write and execute rights while everyone else gets read and execute rights. This is important so that the generated script can be executed. Additionally the depends_on argument is set to mark this file dependent on the known_hosts file. It will only be generated after the known_hosts file has been generated and respectively will be destroyed before the known_hosts file will be destroyed to ensure the depenedency chain. The two files can now be used to transfer files between the local machine and the server and connect in a very convenient way, all without having to mess with any fingerprints again: $ ./bin/scp.sh ./main.tf devops@95.216.223.223:~/test.tf main.tf 100% 2625 65.5KB/s 00:00 $ ./bin/ssh.sh Linux exercise-14 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 ... devops@exercise-14:~$ ls test.tf devops@exercise-14:~$","title":"Generating Wrappers around ssh and scp"},{"location":"exercise15/","text":"Exercise 15: Partitions and mounting Click here to view the solution in the repository. To add a volume into the Terraform configuration a hcloud_volume resource is created. When the server_id is specified it will be automatically attached to that server. Alternatively a hcloud_volume_attachment can be used: resource \"hcloud_volume\" \"volume_exercise_15\" { name = \"volume-exercise-15\" size = 10 server_id = hcloud_server.exercise_15.id automount = true format = \"xfs\" } output \"volume_device\" { description = \"Linux device name of the volume\" value = hcloud_volume.volume_exercise_15.linux_device } The automount argument makes the volume being automatically mounted into the file system upon attaching it to the server. This feature is currently broken, since it is being implemented by a runcmd directive in cloud-init which is overwritten by the custom cloud-init configuration provided. To still make the automount feature available, the command used to automount is just added to the top of the runcmd list in the custom cloud-init file: ... runcmd: - udevadm trigger -c add -s block -p ID_VENDOR=HC --verbose -p ID_MODEL=Volume ... When connecting to the server now the mounted volume can be observed under /dev/sdb in the file tree. The name output by Terraform says the volume is named /dev/disk/by-id/scsi-0HC_Volume_102984619 . Linux by default has all of the disks connected to the system available under /dev . Each disk and partition connected has a so called \u201cblock special file\u201d or \u201cblock device file\u201d that refers to the block device. /dev/sda , /dev/sdb , \u2026 are just naming conventions for the different disk\u2019s block files. The file name given by Hetzner to the created volume ( linux_device ) is static. The connection between these two is that the linux_device name ( /dev/disk/by-id/scsi-0HC_Volume_102984619 ) is a symlink to the actual block file ( /dev/sdb ) of the volume. This means that if the location of the file were to change (when attaching more volumes for example) the same volume may not be reachable under /dev/sdb anymore but under /dev/sdc for example. But the linux_device path /dev/disk/by-id/scsi-0HC_Volume_102984619 would still be the same and not change regardless of the actual file location: $ df -h Filesystem Size Used Avail Use% Mounted on ... /dev/sdb 10G 104M 9.9G 2% /mnt/HC_Volume_102984619 $ ls -l /dev/disk/by-id/ total 0 lrwxrwxrwx 1 root root 9 Jul 29 15:59 ata-QEMU_DVD-ROM_QM00001 -> ../../sr0 lrwxrwxrwx 1 root root 9 Jul 29 15:59 scsi-0HC_Volume_102984619 -> ../../sdb ... Unmounting the Volume As can be seen in the output of the df command above the volume is mounted under /mnt/HC_Volume_102984619 . When trying to unmount the volume while being inside of it the unmounting fails with a target is busy message which seems reasonable since we are currently using the disk: devops@exercise-15:/$ cd /mnt/HC_Volume_102984619/ devops@exercise-15:/mnt/HC_Volume_102984619$ sudo umount /mnt/HC_Volume_102984619 umount: /mnt/HC_Volume_102984619: target is busy. When leaving the mounted volume and trying to unmount again the operation is successful. Since there is no one using the device anymore it is no longer blocked and can be unmounted. If also won\u2019t show up in the list of file systems: devops@exercise-15:/$ sudo umount /mnt/HC_Volume_102984619 devops@exercise-15:/$ df -h Filesystem Size Used Avail Use% Mounted on udev 938M 0 938M 0% /dev tmpfs 192M 680K 192M 1% /run /dev/sda1 38G 1.6G 35G 5% / tmpfs 960M 0 960M 0% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock /dev/sda15 241M 138K 241M 1% /boot/efi tmpfs 192M 0 192M 0% /run/user/1000 Partitioning the Volume To partition the volume fdisk is used. fdisk is an interactive program to create and manipulate partitions. To modify the /dev/sdb device where the disk is currently attached sudo fdisk /dev/sdb is used. Since the new volume is not partitioned yet a partition table needs to be created. We can confirm that the device is not partitioned yet with the command F which lists unpartitioned space: Command (m for help): F Unpartitioned space /dev/sdb: 10 GiB, 10736352768 bytes, 20969439 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes Start End Sectors Size 2048 20971486 20969439 10G With g a new GPT partition table is created. We can now add new partitions to the disk with the command n . For each partition the partition number, the start ( first sector ) and the end ( last sector ) need to be specified. With +5G we can make the last sector automatically be at around 5 GB after the start sector, splitting the disk in about half: Command (m for help): g Created a new GPT disklabel (GUID: 63C1C8E4-1B4F-7E45-8FF3-E02FD3EDDEF2). The device contains 'xfs' signature and it will be removed by a write command. See fdisk(8) man page and --wipe option for more details. Command (m for help): n Partition number (1-128, default 1): 1 First sector (2048-20971486, default 2048): Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-20971486, default 20969471): +5G Created a new partition 1 of type 'Linux filesystem' and of size 5 GiB. Command (m for help): n Partition number (2-128, default 2): First sector (10487808-20971486, default 10487808): Last sector, +/-sectors or +/-size{K,M,G,T,P} (10487808-20971486, default 20969471): Created a new partition 2 of type 'Linux filesystem' and of size 5 GiB. We can inspect and confirm the partitions created by using the p command to display the partition table before committing the changes using w : Command (m for help): p ... Device Start End Sectors Size Type /dev/sdb1 2048 10487807 10485760 5G Linux filesystem /dev/sdb2 10487808 20969471 10481664 5G Linux filesystem Command (m for help): w The partition table has been altered. Calling ioctl() to re-read partition table. Syncing disks. Both partitions created are currently just empty. In order to actually be able to use them to save files, a file system must be initialized on them. The ext4 file system is used on the first partition while xfs is used on the second one: $ sudo mkfs -t ext4 /dev/sdb1 ... Writing superblocks and filesystem accounting information: done devops@exercise-15:/$ sudo mkfs -t xfs /dev/sdb2 ... Discarding blocks...Done. Right now those file systems are not mounted to the current file system of the server. In order to do that mounting points ( /disk1 and /disk2 ) have to be created and then partitions can be mounted to that location. File systems can be mounted using both the partition\u2019s block file as well as the identfier: devops@exercise-15:/$ sudo blkid ... /dev/sdb2: UUID=\"2a2be3e2-3b0c-4370-980b-0113c6faa901\" BLOCK_SIZE=\"512\" TYPE=\"xfs\" PARTUUID=\"833b533a-10eb-8245-8e31-a0da5e13d93a\" ... devops@exercise-15:/$ sudo mount /dev/sdb1 /disk1 devops@exercise-15:/$ sudo mount UUID=2a2be3e2-3b0c-4370-980b-0113c6faa901 /disk2 devops@exercise-15:/$ df -h Filesystem Size Used Avail Use% Mounted on ... /dev/sdb1 4.9G 24K 4.6G 1% /disk1 /dev/sdb2 5.0G 68M 4.9G 2% /disk2 When unmounting a file system from the main file system all it\u2019s content becomes unavailable: devops@exercise-15:/disk1$ sudo touch test devops@exercise-15:/disk1$ ls lost+found test devops@exercise-15:/disk1$ cd .. devops@exercise-15:/$ sudo umount /disk1 devops@exercise-15:/$ sudo umount /disk2 devops@exercise-15:/$ cd /disk1 devops@exercise-15:/disk1$ ls devops@exercise-15:/disk1$ Making Mounts Permanent The file /etc/fstab contains information about mountable file systems in the system. Every line in the file represents a file system that can be mounted and is a list of six whitespace-separated fields. The first field is the specification of the file system to be mounted. It can be the name of the block device file, a UUID or other values. In case of the first file system it would be /dev/sdb1 , for the second UUID=2a2be3e2-3b0c-4370-980b-0113c6faa901 . It is usually better to specify the UUID of the file system since as explored earlier the block file may change and then invalidate the fstab file. The second field is the target path in the root file system. These are /disk1 and /disk2 respectively. The third field contains the type of file system of the mounted file system. It is ext4 and xfs . The fourth field contains mout options. There it can be specified how and when the file system should be mounted. Since we want the file systems to be mounted on boot, the auto option is used to indicate that. It is also a good idea to at least specify the defaults option which also applies a prediefined, kernel-specific set of options. Additional options will override those defaults. The fifth field is connected to the dumping of file systems. 0 means no dubping which is the used option. The sixth field contains the order in which the file systems are checked by fsck . A value of 0 excludes the file system from such a check, a value of 1 should only be used for the root file system and a value of 2 (which has been chosen) for the rest. The two /etc/fstab entries for the two file system mounts now look like this: /dev/sdb1 /disk1 ext4 auto,defaults 0 2 UUID=2a2be3e2-3b0c-4370-980b-0113c6faa901 /disk2 xfs auto,defaults 0 2 After a reboot both file systems are mounted: devops@exercise-15:/$ sudo reboot Broadcast message from root@exercise-15 on pts/1 (Tue 2025-07-29 17:27:10 UTC): The system will reboot now! ... devops@exercise-15:~$ df -h Filesystem Size Used Avail Use% Mounted on ... /dev/sdb1 4.9G 24K 4.6G 1% /disk1 /dev/sdb2 5.0G 68M 4.9G 2% /disk2","title":"Exercise 15"},{"location":"exercise15/#exercise-15-partitions-and-mounting","text":"Click here to view the solution in the repository. To add a volume into the Terraform configuration a hcloud_volume resource is created. When the server_id is specified it will be automatically attached to that server. Alternatively a hcloud_volume_attachment can be used: resource \"hcloud_volume\" \"volume_exercise_15\" { name = \"volume-exercise-15\" size = 10 server_id = hcloud_server.exercise_15.id automount = true format = \"xfs\" } output \"volume_device\" { description = \"Linux device name of the volume\" value = hcloud_volume.volume_exercise_15.linux_device } The automount argument makes the volume being automatically mounted into the file system upon attaching it to the server. This feature is currently broken, since it is being implemented by a runcmd directive in cloud-init which is overwritten by the custom cloud-init configuration provided. To still make the automount feature available, the command used to automount is just added to the top of the runcmd list in the custom cloud-init file: ... runcmd: - udevadm trigger -c add -s block -p ID_VENDOR=HC --verbose -p ID_MODEL=Volume ... When connecting to the server now the mounted volume can be observed under /dev/sdb in the file tree. The name output by Terraform says the volume is named /dev/disk/by-id/scsi-0HC_Volume_102984619 . Linux by default has all of the disks connected to the system available under /dev . Each disk and partition connected has a so called \u201cblock special file\u201d or \u201cblock device file\u201d that refers to the block device. /dev/sda , /dev/sdb , \u2026 are just naming conventions for the different disk\u2019s block files. The file name given by Hetzner to the created volume ( linux_device ) is static. The connection between these two is that the linux_device name ( /dev/disk/by-id/scsi-0HC_Volume_102984619 ) is a symlink to the actual block file ( /dev/sdb ) of the volume. This means that if the location of the file were to change (when attaching more volumes for example) the same volume may not be reachable under /dev/sdb anymore but under /dev/sdc for example. But the linux_device path /dev/disk/by-id/scsi-0HC_Volume_102984619 would still be the same and not change regardless of the actual file location: $ df -h Filesystem Size Used Avail Use% Mounted on ... /dev/sdb 10G 104M 9.9G 2% /mnt/HC_Volume_102984619 $ ls -l /dev/disk/by-id/ total 0 lrwxrwxrwx 1 root root 9 Jul 29 15:59 ata-QEMU_DVD-ROM_QM00001 -> ../../sr0 lrwxrwxrwx 1 root root 9 Jul 29 15:59 scsi-0HC_Volume_102984619 -> ../../sdb ...","title":"Exercise 15: Partitions and mounting"},{"location":"exercise15/#unmounting-the-volume","text":"As can be seen in the output of the df command above the volume is mounted under /mnt/HC_Volume_102984619 . When trying to unmount the volume while being inside of it the unmounting fails with a target is busy message which seems reasonable since we are currently using the disk: devops@exercise-15:/$ cd /mnt/HC_Volume_102984619/ devops@exercise-15:/mnt/HC_Volume_102984619$ sudo umount /mnt/HC_Volume_102984619 umount: /mnt/HC_Volume_102984619: target is busy. When leaving the mounted volume and trying to unmount again the operation is successful. Since there is no one using the device anymore it is no longer blocked and can be unmounted. If also won\u2019t show up in the list of file systems: devops@exercise-15:/$ sudo umount /mnt/HC_Volume_102984619 devops@exercise-15:/$ df -h Filesystem Size Used Avail Use% Mounted on udev 938M 0 938M 0% /dev tmpfs 192M 680K 192M 1% /run /dev/sda1 38G 1.6G 35G 5% / tmpfs 960M 0 960M 0% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock /dev/sda15 241M 138K 241M 1% /boot/efi tmpfs 192M 0 192M 0% /run/user/1000","title":"Unmounting the Volume"},{"location":"exercise15/#partitioning-the-volume","text":"To partition the volume fdisk is used. fdisk is an interactive program to create and manipulate partitions. To modify the /dev/sdb device where the disk is currently attached sudo fdisk /dev/sdb is used. Since the new volume is not partitioned yet a partition table needs to be created. We can confirm that the device is not partitioned yet with the command F which lists unpartitioned space: Command (m for help): F Unpartitioned space /dev/sdb: 10 GiB, 10736352768 bytes, 20969439 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes Start End Sectors Size 2048 20971486 20969439 10G With g a new GPT partition table is created. We can now add new partitions to the disk with the command n . For each partition the partition number, the start ( first sector ) and the end ( last sector ) need to be specified. With +5G we can make the last sector automatically be at around 5 GB after the start sector, splitting the disk in about half: Command (m for help): g Created a new GPT disklabel (GUID: 63C1C8E4-1B4F-7E45-8FF3-E02FD3EDDEF2). The device contains 'xfs' signature and it will be removed by a write command. See fdisk(8) man page and --wipe option for more details. Command (m for help): n Partition number (1-128, default 1): 1 First sector (2048-20971486, default 2048): Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-20971486, default 20969471): +5G Created a new partition 1 of type 'Linux filesystem' and of size 5 GiB. Command (m for help): n Partition number (2-128, default 2): First sector (10487808-20971486, default 10487808): Last sector, +/-sectors or +/-size{K,M,G,T,P} (10487808-20971486, default 20969471): Created a new partition 2 of type 'Linux filesystem' and of size 5 GiB. We can inspect and confirm the partitions created by using the p command to display the partition table before committing the changes using w : Command (m for help): p ... Device Start End Sectors Size Type /dev/sdb1 2048 10487807 10485760 5G Linux filesystem /dev/sdb2 10487808 20969471 10481664 5G Linux filesystem Command (m for help): w The partition table has been altered. Calling ioctl() to re-read partition table. Syncing disks. Both partitions created are currently just empty. In order to actually be able to use them to save files, a file system must be initialized on them. The ext4 file system is used on the first partition while xfs is used on the second one: $ sudo mkfs -t ext4 /dev/sdb1 ... Writing superblocks and filesystem accounting information: done devops@exercise-15:/$ sudo mkfs -t xfs /dev/sdb2 ... Discarding blocks...Done. Right now those file systems are not mounted to the current file system of the server. In order to do that mounting points ( /disk1 and /disk2 ) have to be created and then partitions can be mounted to that location. File systems can be mounted using both the partition\u2019s block file as well as the identfier: devops@exercise-15:/$ sudo blkid ... /dev/sdb2: UUID=\"2a2be3e2-3b0c-4370-980b-0113c6faa901\" BLOCK_SIZE=\"512\" TYPE=\"xfs\" PARTUUID=\"833b533a-10eb-8245-8e31-a0da5e13d93a\" ... devops@exercise-15:/$ sudo mount /dev/sdb1 /disk1 devops@exercise-15:/$ sudo mount UUID=2a2be3e2-3b0c-4370-980b-0113c6faa901 /disk2 devops@exercise-15:/$ df -h Filesystem Size Used Avail Use% Mounted on ... /dev/sdb1 4.9G 24K 4.6G 1% /disk1 /dev/sdb2 5.0G 68M 4.9G 2% /disk2 When unmounting a file system from the main file system all it\u2019s content becomes unavailable: devops@exercise-15:/disk1$ sudo touch test devops@exercise-15:/disk1$ ls lost+found test devops@exercise-15:/disk1$ cd .. devops@exercise-15:/$ sudo umount /disk1 devops@exercise-15:/$ sudo umount /disk2 devops@exercise-15:/$ cd /disk1 devops@exercise-15:/disk1$ ls devops@exercise-15:/disk1$","title":"Partitioning the Volume"},{"location":"exercise15/#making-mounts-permanent","text":"The file /etc/fstab contains information about mountable file systems in the system. Every line in the file represents a file system that can be mounted and is a list of six whitespace-separated fields. The first field is the specification of the file system to be mounted. It can be the name of the block device file, a UUID or other values. In case of the first file system it would be /dev/sdb1 , for the second UUID=2a2be3e2-3b0c-4370-980b-0113c6faa901 . It is usually better to specify the UUID of the file system since as explored earlier the block file may change and then invalidate the fstab file. The second field is the target path in the root file system. These are /disk1 and /disk2 respectively. The third field contains the type of file system of the mounted file system. It is ext4 and xfs . The fourth field contains mout options. There it can be specified how and when the file system should be mounted. Since we want the file systems to be mounted on boot, the auto option is used to indicate that. It is also a good idea to at least specify the defaults option which also applies a prediefined, kernel-specific set of options. Additional options will override those defaults. The fifth field is connected to the dumping of file systems. 0 means no dubping which is the used option. The sixth field contains the order in which the file systems are checked by fsck . A value of 0 excludes the file system from such a check, a value of 1 should only be used for the root file system and a value of 2 (which has been chosen) for the rest. The two /etc/fstab entries for the two file system mounts now look like this: /dev/sdb1 /disk1 ext4 auto,defaults 0 2 UUID=2a2be3e2-3b0c-4370-980b-0113c6faa901 /disk2 xfs auto,defaults 0 2 After a reboot both file systems are mounted: devops@exercise-15:/$ sudo reboot Broadcast message from root@exercise-15 on pts/1 (Tue 2025-07-29 17:27:10 UTC): The system will reboot now! ... devops@exercise-15:~$ df -h Filesystem Size Used Avail Use% Mounted on ... /dev/sdb1 4.9G 24K 4.6G 1% /disk1 /dev/sdb2 5.0G 68M 4.9G 2% /disk2","title":"Making Mounts Permanent"},{"location":"exercise16/","text":"Exercise 16: Mount point\u2019s name specification Click here to view the solution in the repository. Instead of handling the mount of the disks manually and using automounted volumes the mounting can also be done manually. This results in much more control over the mounting process than when using automount. The process is largely the same as in the previous exercise but done automatically. To make the architecture configuration more customizable two new variables are introduced: location and volume_mount . variable \"volume_mount\" { description = \"Mounting point of the volume in the root file system\" type = string default = \"volume01\" } variable \"location\" { description = \"Hetzner datacenter location to create the main resources in\" type = string default = \"hel1\" validation { condition = contains([\"fsn1\", \"nbg1\", \"hel1\", \"ash\", \"hil\", \"sin\"], var.location) error_message = \"The location must be a valid datacenter\" } } location is used to control the datacenter of both the server and the volume. Without specifying a common datacenter the volume and the server may be created in datacenters to Hetzners discretion. They will want the created architecture to be always balanced so if one datacenter has a high capacity of volumes, new volumes will more likely be created there if no explicit location is specified. This problem was not occurring when automounting was enabled since the volume was directly attached to a server. To ensure valid values are used a custom validation is added to the location variable to ensure the value specified is one of Hetzner\u2019s datacenters . The volume_mount variable is used to specify the mounting path of the attached volume in the root file system. It is used by the cloud-init script to create and mount at the correct location. Since the volume creation is now decoupled from the server creation the architecture has to be modified: resource \"hcloud_volume\" \"volume_exercise_16\" { name = \"volume-exercise-16\" size = 10 location = var.location format = \"xfs\" } resource \"hcloud_server\" \"exercise_16\" { name = \"exercise-16\" image = \"debian-12\" server_type = \"cpx11\" location = var.location user_data = local_file.cloud_init.content } resource \"hcloud_volume_attachment\" \"exercise_volume_attachment\" { volume_id = hcloud_volume.volume_exercise_16.id server_id = hcloud_server.exercise_16.id automount = false lifecycle { replace_triggered_by = [hcloud_server.exercise_16, hcloud_volume.volume_exercise_16] } } The cloud-init script is now responsible for mounting the volume to the correct location on the server. For that the entry in /etc/fstab has to be created and then the mount has to be triggered via mount -a : runcmd: ... - mkdir /${volume_mount} - echo ${volume_device} /${volume_mount} xfs auto,rw,defaults 0 2 >> /etc/fstab - systemctl daemon-reload - mount -a ... When trying this out the resulting file system was usually not mounted. This could be because the mount -a command is executed too early, resulting in some other processes to still be busy with the changes made to the mounting system. It may also be possible that the volume attachment used to attach the volume to the server takes longer than expected. It is being created after the creation of both server and volume which would make it a likely failure point. To resolve this issue the call to mount -a has been moved to the end of the runcmd section after a sleep , giving the system enough time to settle before mounting the volume: runcmd: ... - mkdir /${volume_mount} - echo ${volume_device} /${volume_mount} xfs auto,rw,defaults 0 2 >> /etc/fstab - systemctl daemon-reload ... - sleep 5 - mount -a Now the volume can correctly be found in the file system after creation of the server: $ ./bin/ssh.sh ... devops@exercise-16:~$ df -h ... /dev/sdb 10G 104M 9.9G 2% /disk","title":"Exercise 16"},{"location":"exercise16/#exercise-16-mount-points-name-specification","text":"Click here to view the solution in the repository. Instead of handling the mount of the disks manually and using automounted volumes the mounting can also be done manually. This results in much more control over the mounting process than when using automount. The process is largely the same as in the previous exercise but done automatically. To make the architecture configuration more customizable two new variables are introduced: location and volume_mount . variable \"volume_mount\" { description = \"Mounting point of the volume in the root file system\" type = string default = \"volume01\" } variable \"location\" { description = \"Hetzner datacenter location to create the main resources in\" type = string default = \"hel1\" validation { condition = contains([\"fsn1\", \"nbg1\", \"hel1\", \"ash\", \"hil\", \"sin\"], var.location) error_message = \"The location must be a valid datacenter\" } } location is used to control the datacenter of both the server and the volume. Without specifying a common datacenter the volume and the server may be created in datacenters to Hetzners discretion. They will want the created architecture to be always balanced so if one datacenter has a high capacity of volumes, new volumes will more likely be created there if no explicit location is specified. This problem was not occurring when automounting was enabled since the volume was directly attached to a server. To ensure valid values are used a custom validation is added to the location variable to ensure the value specified is one of Hetzner\u2019s datacenters . The volume_mount variable is used to specify the mounting path of the attached volume in the root file system. It is used by the cloud-init script to create and mount at the correct location. Since the volume creation is now decoupled from the server creation the architecture has to be modified: resource \"hcloud_volume\" \"volume_exercise_16\" { name = \"volume-exercise-16\" size = 10 location = var.location format = \"xfs\" } resource \"hcloud_server\" \"exercise_16\" { name = \"exercise-16\" image = \"debian-12\" server_type = \"cpx11\" location = var.location user_data = local_file.cloud_init.content } resource \"hcloud_volume_attachment\" \"exercise_volume_attachment\" { volume_id = hcloud_volume.volume_exercise_16.id server_id = hcloud_server.exercise_16.id automount = false lifecycle { replace_triggered_by = [hcloud_server.exercise_16, hcloud_volume.volume_exercise_16] } } The cloud-init script is now responsible for mounting the volume to the correct location on the server. For that the entry in /etc/fstab has to be created and then the mount has to be triggered via mount -a : runcmd: ... - mkdir /${volume_mount} - echo ${volume_device} /${volume_mount} xfs auto,rw,defaults 0 2 >> /etc/fstab - systemctl daemon-reload - mount -a ... When trying this out the resulting file system was usually not mounted. This could be because the mount -a command is executed too early, resulting in some other processes to still be busy with the changes made to the mounting system. It may also be possible that the volume attachment used to attach the volume to the server takes longer than expected. It is being created after the creation of both server and volume which would make it a likely failure point. To resolve this issue the call to mount -a has been moved to the end of the runcmd section after a sleep , giving the system enough time to settle before mounting the volume: runcmd: ... - mkdir /${volume_mount} - echo ${volume_device} /${volume_mount} xfs auto,rw,defaults 0 2 >> /etc/fstab - systemctl daemon-reload ... - sleep 5 - mount -a Now the volume can correctly be found in the file system after creation of the server: $ ./bin/ssh.sh ... devops@exercise-16:~$ df -h ... /dev/sdb 10G 104M 9.9G 2% /disk","title":"Exercise 16: Mount point\u2019s name specification"},{"location":"exercise17/","text":"Exercise 17: A module for ssh host key handling Click here to view the solution in the repository. Currently, the setup of the known_hosts file and the ssh and scp wrappers is in the main configuration. For simpler development and more modular use it is sensible to move this encapsulated behavior to a module. We can then call this module from multiple configurations or easily loop it to allow for multiple servers to be defined. Moving the File Generation to the Module To move the functionality into a module all the template files, and local_file resources used to generate the known_hosts file and wrappers is moved into ./modules/ssh_wrapper while the rest of the configuration is moved to ./configuration . In the current state the module would not work correctly, since the resources in the modules take values from the resources in the main configuration like the server IP or the SSH public key. This information that the module needs to work as an independent, isolated Terraform module can be specified using variables. These variables will be enforced by Terraform when calling the module. The variables server_username , server_hostname and server_public_key are defined in the child module: variable \"server_username\" { description = \"Username to use for the default user on the server\" type = string } variable \"server_hostname\" { description = \"Hostname to use for the default user on the server (e.g. IPv4)\" type = string } variable \"server_public_key\" { description = \"The SSH public key of the server\" type = string } They will then be used by the resources inside the module to construct the file like in Exercise 14 before. The only difference is the resolution of paths. Before, a relative path like ./gen/known_hosts was enough to specify the location of the file. Now ./gen/known_hosts would generate the file in the gen folder of the child module, not the parent module. Terraforms path resource allows us to circumvent this problem: path.root contains the file path to the root module, which is the main configuration: resource \"local_file\" \"known_hosts\" { filename = \"${path.root}/gen/known_hosts\" content = join(\" \", [ var.server_hostname, var.server_public_key ]) file_permission = \"644\" } resource \"local_file\" \"ssh_bin\" { filename = \"${path.root}/bin/ssh.sh\" ... } resource \"local_file\" \"scp_bin\" { filename = \"${path.root}/bin/scp.sh\" ... } Specifying Paths in More Detail The approach shown above to file paths is robust but not very flexible. It allows the parent module little control over the location of the files, which can lead to issues. Looking forward to Exercise 21 for example, there will be a number of servers and therefor a number of known_host , ssh and scp files generated. If the current setup were to be used, all those files would overwrite each other. It is therefor better to give the calling module control over the location of the files. This can be done using an input variable which contains the target paths of the files being created. When calling the module in a loop later, this will result in each iteration being able to specify their own location separate from each other: variable \"output_dir\" { description = \"The output directory of the files; they will be generated under /gen and /bin inside this directory\" type = string default = path.root } The parent module can now specify the file path: module \"ssh_wrapper\" { source = \"../modules/ssh_wrapper\" server_hostname = hcloud_server.exercise_17.ipv4_address server_username = var.server_username server_public_key = tls_private_key.server_ssh_key.public_key_openssh } With that the same functionality has been extracted into a module: $ ./bin/ssh.sh ... devops@exercise-17:~$","title":"Exercise 17"},{"location":"exercise17/#exercise-17-a-module-for-ssh-host-key-handling","text":"Click here to view the solution in the repository. Currently, the setup of the known_hosts file and the ssh and scp wrappers is in the main configuration. For simpler development and more modular use it is sensible to move this encapsulated behavior to a module. We can then call this module from multiple configurations or easily loop it to allow for multiple servers to be defined.","title":"Exercise 17: A module for ssh host key handling"},{"location":"exercise17/#moving-the-file-generation-to-the-module","text":"To move the functionality into a module all the template files, and local_file resources used to generate the known_hosts file and wrappers is moved into ./modules/ssh_wrapper while the rest of the configuration is moved to ./configuration . In the current state the module would not work correctly, since the resources in the modules take values from the resources in the main configuration like the server IP or the SSH public key. This information that the module needs to work as an independent, isolated Terraform module can be specified using variables. These variables will be enforced by Terraform when calling the module. The variables server_username , server_hostname and server_public_key are defined in the child module: variable \"server_username\" { description = \"Username to use for the default user on the server\" type = string } variable \"server_hostname\" { description = \"Hostname to use for the default user on the server (e.g. IPv4)\" type = string } variable \"server_public_key\" { description = \"The SSH public key of the server\" type = string } They will then be used by the resources inside the module to construct the file like in Exercise 14 before. The only difference is the resolution of paths. Before, a relative path like ./gen/known_hosts was enough to specify the location of the file. Now ./gen/known_hosts would generate the file in the gen folder of the child module, not the parent module. Terraforms path resource allows us to circumvent this problem: path.root contains the file path to the root module, which is the main configuration: resource \"local_file\" \"known_hosts\" { filename = \"${path.root}/gen/known_hosts\" content = join(\" \", [ var.server_hostname, var.server_public_key ]) file_permission = \"644\" } resource \"local_file\" \"ssh_bin\" { filename = \"${path.root}/bin/ssh.sh\" ... } resource \"local_file\" \"scp_bin\" { filename = \"${path.root}/bin/scp.sh\" ... }","title":"Moving the File Generation to the Module"},{"location":"exercise17/#specifying-paths-in-more-detail","text":"The approach shown above to file paths is robust but not very flexible. It allows the parent module little control over the location of the files, which can lead to issues. Looking forward to Exercise 21 for example, there will be a number of servers and therefor a number of known_host , ssh and scp files generated. If the current setup were to be used, all those files would overwrite each other. It is therefor better to give the calling module control over the location of the files. This can be done using an input variable which contains the target paths of the files being created. When calling the module in a loop later, this will result in each iteration being able to specify their own location separate from each other: variable \"output_dir\" { description = \"The output directory of the files; they will be generated under /gen and /bin inside this directory\" type = string default = path.root } The parent module can now specify the file path: module \"ssh_wrapper\" { source = \"../modules/ssh_wrapper\" server_hostname = hcloud_server.exercise_17.ipv4_address server_username = var.server_username server_public_key = tls_private_key.server_ssh_key.public_key_openssh } With that the same functionality has been extracted into a module: $ ./bin/ssh.sh ... devops@exercise-17:~$","title":"Specifying Paths in More Detail"},{"location":"exercise18/","text":"Exercise 18: Enhancing your web server Click here to view the solution in the repository. Adding a DNS Entry for the Web Server Reaching the web server using the IP is possible but not very convenient. To be able to use a normal URL a DNS record for that URL has to be created in a DNS server which will then tell the browser (or other applications) what the IP address of a given URL resolves to. In this exercise this is done manually in order to get an understanding of the workings of the DNS process. To change a DNS entry there are two requirements: The secret key of the zone that is being edited (in this case g2.sdi.hdm-stuttgart.cloud. ) and the IP of the server we want to edit DNS entries for. The secret key is provided from the Moodle course and is loaded into the environment for easier handling: $ export HMAC=hmac-sha512:g2.key:kEndmvYk... $ echo $HMAC hmac-sha512:g2.key:kEndmvYk... The IP can be retrieved as an output of the terraform apply : $ terraform apply ... Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"37.27.219.19\" volume_device = \"/dev/disk/by-id/scsi-0HC_Volume_102991148\" With both of thos requirements satisfied a DNS record can be created. There are multiple different types of DNS records. The most simple form of them is a DNS A record . It links a domain name to an IP address and consists of 4 parts: The domain name, a time to live, the type of record ( A ) and the IP the domain resolves to. The time to live indicates to any machine or application that retrieves this record for how long it is valid and should be cached. Computers usually don't request DNS resolution from the DNS servers all the time but only if a domain unknown to them is being requested. The time to live is the time the record remains \"known\" to the computer before it is re-requested. If the record is updated with a new IP during this time, the update won't propagate to the computer as long as the record has not yet expired, only after that is the record being requested again and the update received. For the purpose of testing and being able to notice updates quickly a TTL of 10 (seconds) is used. The program nsupdate can be used to interactively add records to a given DNS server. In this case two A records are being created: One for www.g2.sdi.hdm-stuttgart.cloud and one for g2.sdi.hdm-stuttgart.cloud both pointing to the IP of the server: $ nsupdate -y $HMAC > server ns1.hdm-stuttgart.cloud > update add www.g2.sdi.hdm-stuttgart.cloud 10 A 37.27.219.19 > update add g2.sdi.hdm-stuttgart.cloud 10 A 37.27.219.19 > send > quit The records can now be accessed when looking up those two domains. The dig utility cen be used to request DNS records from a DNS server. The lookup from the ns1.hdm-stuttgart.cloud DNS server that was used to register the records is successful. It is also interesting to note that the entries can also be found by requesting the domain from Google's DNS server ( 8.8.8.8 ) even though only the HdM DNS server has been updated. This is due to the structure of the DNS system: It is possible for another DNS server to have control over a specific domain. In the \"parent\" DNS server there then is a NS record which will redirect all DNS requests of this domain (or subdomains) to the DNS server stated in the record. $ dig +noall +answer @ns1.hdm-stuttgart.cloud www.g2.sdi.hdm-stuttgart.cloud www.g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 $ dig +noall +answer @ns1.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 $ dig +noall +answer @8.8.8.8 g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 This results in the website hosted by our server not only being reachable at its IP but also via the domain http://www.g2.sdi.hdm-stuttgart.cloud/ / http://g2.sdi.hdm-stuttgart.cloud/ : Adding a TLS Certificate for HTTPS Access Currently the website is served over HTTP which is unencryped - meaning that request and response are transmitted in clear text over the internet. This is insecure for obvious reasons and most browsers today will per default block the access to domains over HTTP. For that reason it is important to secure the traffic to our domain using a TLS certificate - allowing the users to reach the website over HTTPS. The non-profit organisation Let's encrypt provides free TLS certificates using an automated API making it easy to secure traffic to a website using HTTPS. In the following section the utility program certbot is used to request, receive and apply such a certificate to our nginx server. The step-by-step guide at digitalocean is followed. The first step is to install certbot and the certbot-nginx extension to be able to automatically apply the acquired certificate on the nginx server using sudo apt -y install certbot python3-certbot-nginx . The next step is move the hosted website to a server block in nginx that corresponds with the domain g2.sdi.hdm-stuttgart.cloud and configure that server block to be served whenever the website is requested over the domain. For that the file /etc/nginx/sites-available/g2.sdi.hdm-stuttgart.cloud is created and in it the server block is defined: devops@exercise-18:~$ cat /etc/nginx/sites-available/g2.sdi.hdm-stuttgart.cloud server { listen 80; listen [::]:80; root /var/www/g2.sdi.hdm-stuttgart.cloud/html; index index.html index.htm index.nginx-debian.html; server_name g2.sdi.hdm-stuttgart.cloud www.g2.sdi.hdm-stuttgart.cloud; location / { try_files $uri $uri/ =404; } } nginx structures websites into sites-available and sites-enabled . sites-available holds all the configurations but they are not applied and actually served - it is basically a playground for configurations where they can be tested and created without any risk of breaking the served sites. If the configuration is ready and the site should be served a symlink to the configuration is created in sites-enabled . nginx will then serve the files that folder. The serve directory is also changes from /var/www/html to /var/www/g2.sdi.hdm-stuttgart.cloud/html where a similar index.html to the default one has been created. In the nginx.conf the server_names_hash_bucket_size line was also uncommented to avoid conflicts regarding the amount of domain names. devops@exercise-18:~$ sudo nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful devops@exercise-18:~$ sudo systemctl restart nginx After successfully restarting nginx the certificate can be requested from Let's encrypt using certbot . To check if the request would be successful and to respect the rate limits of Let's encrypt a dry run is done first using the --test-cert option. This requests a certificate from the staging environment of Let's encrypt which is not connected to the main certificate authority. devops@exercise-18:~$ sudo certbot --nginx --test-cert -d g2.sdi.hdm-stuttgart.cloud -d www.g2.sdi.hdm-stuttgart.cloud Saving debug log to /var/log/letsencrypt/letsencrypt.log Requesting a certificate for g2.sdi.hdm-stuttgart.cloud and www.g2.sdi.hdm-stuttgart.cloud Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/fullchain.pem Key is saved at: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/privkey.pem This certificate expires on 2025-10-28. These files will be updated when the certificate renews. Certbot has set up a scheduled task to automatically renew this certificate in the background. Deploying certificate Successfully deployed certificate for g2.sdi.hdm-stuttgart.cloud to /etc/nginx/sites-enabled/g2.sdi.hdm-stuttgart.cloud Successfully deployed certificate for www.g2.sdi.hdm-stuttgart.cloud to /etc/nginx/sites-enabled/g2.sdi.hdm-stuttgart.cloud Congratulations! You have successfully enabled HTTPS on https://g2.sdi.hdm-stuttgart.cloud and https://www.g2.sdi.hdm-stuttgart.cloud After the test run has run successfully the --test-cert option can be omitted to request the real certificate. The on the test run received certificate has to be replaced with the real one. After that the TLS certificate is successfully installed and certbot also configures the nginx server block for the domain correctly. devops@exercise-18:~$ sudo certbot --nginx -d g2.sdi.hdm-stuttgart.cloud -d www.g2.sdi.hdm-stuttgart.cloud Saving debug log to /var/log/letsencrypt/letsencrypt.log Certificate not yet due for renewal You have an existing certificate that has exactly the same domains or certificate name you requested and isn't close to expiry. (ref: /etc/letsencrypt/renewal/g2.sdi.hdm-stuttgart.cloud.conf) What would you like to do? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: Attempt to reinstall this existing certificate 2: Renew & replace the certificate (may be subject to CA rate limits) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 2 Renewing an existing certificate for g2.sdi.hdm-stuttgart.cloud and www.g2.sdi.hdm-stuttgart.cloud Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/fullchain.pem Key is saved at: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/privkey.pem This certificate expires on 2025-10-28. These files will be updated when the certificate renews. Certbot has set up a scheduled task to automatically renew this certificate in the background. Deploying certificate Successfully deployed certificate for g2.sdi.hdm-stuttgart.cloud to /etc/nginx/sites-enabled/g2.sdi.hdm-stuttgart.cloud Successfully deployed certificate for www.g2.sdi.hdm-stuttgart.cloud to /etc/nginx/sites-enabled/g2.sdi.hdm-stuttgart.cloud Your existing certificate has been successfully renewed, and the new certificate has been installed. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - If you like Certbot, please consider supporting our work by: * Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate * Donating to EFF: https://eff.org/donate-le - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - In the end it can also be useful to check if the renewals of the certificate using certbot is working normally. For this the --dry-run option can be specified: devops@exercise-18:~$ sudo certbot renew --dry-run Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Processing /etc/letsencrypt/renewal/g2.sdi.hdm-stuttgart.cloud.conf - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Simulating renewal of an existing certificate for g2.sdi.hdm-stuttgart.cloud and www.g2.sdi.hdm-stuttgart.cloud - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Congratulations, all simulated renewals succeeded: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/fullchain.pem (success) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - With that the website is successfully encrypted and can be reached over HTTPS after a respective firewall rule is added to the server's firewall: resource \"hcloud_firewall\" \"fw_exercise_18\" { name = \"exercise-18-fw\" ... rule { description = \"HTTPS inbound\" direction = \"in\" protocol = \"tcp\" port = 443 source_ips = [\"0.0.0.0/0\", \"::/0\"] } }","title":"Exercise 18"},{"location":"exercise18/#exercise-18-enhancing-your-web-server","text":"Click here to view the solution in the repository.","title":"Exercise 18: Enhancing your web server"},{"location":"exercise18/#adding-a-dns-entry-for-the-web-server","text":"Reaching the web server using the IP is possible but not very convenient. To be able to use a normal URL a DNS record for that URL has to be created in a DNS server which will then tell the browser (or other applications) what the IP address of a given URL resolves to. In this exercise this is done manually in order to get an understanding of the workings of the DNS process. To change a DNS entry there are two requirements: The secret key of the zone that is being edited (in this case g2.sdi.hdm-stuttgart.cloud. ) and the IP of the server we want to edit DNS entries for. The secret key is provided from the Moodle course and is loaded into the environment for easier handling: $ export HMAC=hmac-sha512:g2.key:kEndmvYk... $ echo $HMAC hmac-sha512:g2.key:kEndmvYk... The IP can be retrieved as an output of the terraform apply : $ terraform apply ... Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"37.27.219.19\" volume_device = \"/dev/disk/by-id/scsi-0HC_Volume_102991148\" With both of thos requirements satisfied a DNS record can be created. There are multiple different types of DNS records. The most simple form of them is a DNS A record . It links a domain name to an IP address and consists of 4 parts: The domain name, a time to live, the type of record ( A ) and the IP the domain resolves to. The time to live indicates to any machine or application that retrieves this record for how long it is valid and should be cached. Computers usually don't request DNS resolution from the DNS servers all the time but only if a domain unknown to them is being requested. The time to live is the time the record remains \"known\" to the computer before it is re-requested. If the record is updated with a new IP during this time, the update won't propagate to the computer as long as the record has not yet expired, only after that is the record being requested again and the update received. For the purpose of testing and being able to notice updates quickly a TTL of 10 (seconds) is used. The program nsupdate can be used to interactively add records to a given DNS server. In this case two A records are being created: One for www.g2.sdi.hdm-stuttgart.cloud and one for g2.sdi.hdm-stuttgart.cloud both pointing to the IP of the server: $ nsupdate -y $HMAC > server ns1.hdm-stuttgart.cloud > update add www.g2.sdi.hdm-stuttgart.cloud 10 A 37.27.219.19 > update add g2.sdi.hdm-stuttgart.cloud 10 A 37.27.219.19 > send > quit The records can now be accessed when looking up those two domains. The dig utility cen be used to request DNS records from a DNS server. The lookup from the ns1.hdm-stuttgart.cloud DNS server that was used to register the records is successful. It is also interesting to note that the entries can also be found by requesting the domain from Google's DNS server ( 8.8.8.8 ) even though only the HdM DNS server has been updated. This is due to the structure of the DNS system: It is possible for another DNS server to have control over a specific domain. In the \"parent\" DNS server there then is a NS record which will redirect all DNS requests of this domain (or subdomains) to the DNS server stated in the record. $ dig +noall +answer @ns1.hdm-stuttgart.cloud www.g2.sdi.hdm-stuttgart.cloud www.g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 $ dig +noall +answer @ns1.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 $ dig +noall +answer @8.8.8.8 g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 This results in the website hosted by our server not only being reachable at its IP but also via the domain http://www.g2.sdi.hdm-stuttgart.cloud/ / http://g2.sdi.hdm-stuttgart.cloud/ :","title":"Adding a DNS Entry for the Web Server"},{"location":"exercise18/#adding-a-tls-certificate-for-https-access","text":"Currently the website is served over HTTP which is unencryped - meaning that request and response are transmitted in clear text over the internet. This is insecure for obvious reasons and most browsers today will per default block the access to domains over HTTP. For that reason it is important to secure the traffic to our domain using a TLS certificate - allowing the users to reach the website over HTTPS. The non-profit organisation Let's encrypt provides free TLS certificates using an automated API making it easy to secure traffic to a website using HTTPS. In the following section the utility program certbot is used to request, receive and apply such a certificate to our nginx server. The step-by-step guide at digitalocean is followed. The first step is to install certbot and the certbot-nginx extension to be able to automatically apply the acquired certificate on the nginx server using sudo apt -y install certbot python3-certbot-nginx . The next step is move the hosted website to a server block in nginx that corresponds with the domain g2.sdi.hdm-stuttgart.cloud and configure that server block to be served whenever the website is requested over the domain. For that the file /etc/nginx/sites-available/g2.sdi.hdm-stuttgart.cloud is created and in it the server block is defined: devops@exercise-18:~$ cat /etc/nginx/sites-available/g2.sdi.hdm-stuttgart.cloud server { listen 80; listen [::]:80; root /var/www/g2.sdi.hdm-stuttgart.cloud/html; index index.html index.htm index.nginx-debian.html; server_name g2.sdi.hdm-stuttgart.cloud www.g2.sdi.hdm-stuttgart.cloud; location / { try_files $uri $uri/ =404; } } nginx structures websites into sites-available and sites-enabled . sites-available holds all the configurations but they are not applied and actually served - it is basically a playground for configurations where they can be tested and created without any risk of breaking the served sites. If the configuration is ready and the site should be served a symlink to the configuration is created in sites-enabled . nginx will then serve the files that folder. The serve directory is also changes from /var/www/html to /var/www/g2.sdi.hdm-stuttgart.cloud/html where a similar index.html to the default one has been created. In the nginx.conf the server_names_hash_bucket_size line was also uncommented to avoid conflicts regarding the amount of domain names. devops@exercise-18:~$ sudo nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful devops@exercise-18:~$ sudo systemctl restart nginx After successfully restarting nginx the certificate can be requested from Let's encrypt using certbot . To check if the request would be successful and to respect the rate limits of Let's encrypt a dry run is done first using the --test-cert option. This requests a certificate from the staging environment of Let's encrypt which is not connected to the main certificate authority. devops@exercise-18:~$ sudo certbot --nginx --test-cert -d g2.sdi.hdm-stuttgart.cloud -d www.g2.sdi.hdm-stuttgart.cloud Saving debug log to /var/log/letsencrypt/letsencrypt.log Requesting a certificate for g2.sdi.hdm-stuttgart.cloud and www.g2.sdi.hdm-stuttgart.cloud Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/fullchain.pem Key is saved at: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/privkey.pem This certificate expires on 2025-10-28. These files will be updated when the certificate renews. Certbot has set up a scheduled task to automatically renew this certificate in the background. Deploying certificate Successfully deployed certificate for g2.sdi.hdm-stuttgart.cloud to /etc/nginx/sites-enabled/g2.sdi.hdm-stuttgart.cloud Successfully deployed certificate for www.g2.sdi.hdm-stuttgart.cloud to /etc/nginx/sites-enabled/g2.sdi.hdm-stuttgart.cloud Congratulations! You have successfully enabled HTTPS on https://g2.sdi.hdm-stuttgart.cloud and https://www.g2.sdi.hdm-stuttgart.cloud After the test run has run successfully the --test-cert option can be omitted to request the real certificate. The on the test run received certificate has to be replaced with the real one. After that the TLS certificate is successfully installed and certbot also configures the nginx server block for the domain correctly. devops@exercise-18:~$ sudo certbot --nginx -d g2.sdi.hdm-stuttgart.cloud -d www.g2.sdi.hdm-stuttgart.cloud Saving debug log to /var/log/letsencrypt/letsencrypt.log Certificate not yet due for renewal You have an existing certificate that has exactly the same domains or certificate name you requested and isn't close to expiry. (ref: /etc/letsencrypt/renewal/g2.sdi.hdm-stuttgart.cloud.conf) What would you like to do? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: Attempt to reinstall this existing certificate 2: Renew & replace the certificate (may be subject to CA rate limits) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 2 Renewing an existing certificate for g2.sdi.hdm-stuttgart.cloud and www.g2.sdi.hdm-stuttgart.cloud Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/fullchain.pem Key is saved at: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/privkey.pem This certificate expires on 2025-10-28. These files will be updated when the certificate renews. Certbot has set up a scheduled task to automatically renew this certificate in the background. Deploying certificate Successfully deployed certificate for g2.sdi.hdm-stuttgart.cloud to /etc/nginx/sites-enabled/g2.sdi.hdm-stuttgart.cloud Successfully deployed certificate for www.g2.sdi.hdm-stuttgart.cloud to /etc/nginx/sites-enabled/g2.sdi.hdm-stuttgart.cloud Your existing certificate has been successfully renewed, and the new certificate has been installed. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - If you like Certbot, please consider supporting our work by: * Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate * Donating to EFF: https://eff.org/donate-le - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - In the end it can also be useful to check if the renewals of the certificate using certbot is working normally. For this the --dry-run option can be specified: devops@exercise-18:~$ sudo certbot renew --dry-run Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Processing /etc/letsencrypt/renewal/g2.sdi.hdm-stuttgart.cloud.conf - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Simulating renewal of an existing certificate for g2.sdi.hdm-stuttgart.cloud and www.g2.sdi.hdm-stuttgart.cloud - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Congratulations, all simulated renewals succeeded: /etc/letsencrypt/live/g2.sdi.hdm-stuttgart.cloud/fullchain.pem (success) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - With that the website is successfully encrypted and can be reached over HTTPS after a respective firewall rule is added to the server's firewall: resource \"hcloud_firewall\" \"fw_exercise_18\" { name = \"exercise-18-fw\" ... rule { description = \"HTTPS inbound\" direction = \"in\" protocol = \"tcp\" port = 443 source_ips = [\"0.0.0.0/0\", \"::/0\"] } }","title":"Adding a TLS Certificate for HTTPS Access"},{"location":"exercise19/","text":"Exercise 19: Creating DNS records Click here to view the solution in the repository. Currently the DNS configuration has only been done manually before in Exercise 18 . If the Terraform configuration is destroyed and then applied the IP may change making all the DNS entries invalid. It is much better to automatically create and update the DNS entries using Terraform. To do that it is ensured that there are no leftover DNS entries in the ns1.hdm-stuttgart.cloud nameserver: $ dig +noall +answer @ns1.hdm-stuttgart.cloud -y $HMAC -t AXFR g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 20 604800 86400 2419200 604800 g2.sdi.hdm-stuttgart.cloud. 600 IN NS ns1.hdm-stuttgart.cloud. g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 20 604800 86400 2419200 604800 To create different DNS records automatically in Terraform the dns provider can be used. It has to be configured with the address and secret key of the server to be able to create, update and delete DNS records. To hold the secret key, a variable has been created that will be injected using a non-versioned secrets.auto.tfvars file: variable \"dns_secret_key\" { description = \"Secret Key for the DNS nameserver\" type = string sensitive = true } provider \"dns\" { update { server = \"ns1.hdm-stuttgart.cloud\" key_name = \"g2.key.\" key_algorithm = \"hmac-sha512\" key_secret = var.dns_secret_key } } In this exercise two types of DNS records should be created: A and CNAME . A records have been discussed in Exercise 18 before. CNAME records are alias records: They point from one domain name to another domain name. The domain they are aliasing will then have to be looked up, following the chain until an IP address is found. The following records are created using Terraform: A base A record with the domain name g2.sdi.hdm-stuttgart.cloud pointing to the server IP A main A record with the name of the server appended to the domain (e.g. workhorse.g2.sdi.hdm-stuttgart.cloud ) also pointing to the server IP A set of CNAME aliases, all pointing to the main A record (e.g. mail.g2.sdi.hdm-stuttgart.cloud and www.g2.sdi.hdm-stuttgart.cloud ) To keep these records as modular as possible all the domains are specified using variables. The base domain, a main server name as well as a set of alias names (entries are automatically unique), which is validated to not contain the main server name: variable \"dns_server_domain\" { description = \"The base domain of the server\" type = string default = \"g2.sdi.hdm-stuttgart.cloud\" } variable \"dns_server_name\" { description = \"The name of the server\" type = string } variable \"dns_server_aliases\" { description = \"Alias names of the server\" type = set(string) default = [] validation { condition = !contains(var.dns_server_aliases, var.dns_server_name) error_message = \"Alias may not shadow the main name\" } } These variables are then used to create the above mentioned set of DNS records. The A records are straightforward. They just take in the domain and the IP address the record is supposed to be pointing to. The CNAME records take in the domain they are aliasing instead of the IP but due to there being a variable amount of aliases the for_each argument is used to create a Terraform loop which executes the resource once for every alias specified. The implicit attribute each.key is then used to get the individual name of each alias. resource \"dns_a_record_set\" \"exercise_dns_base_record\" { zone = \"${var.dns_server_domain}.\" addresses = [hcloud_server.exercise_19.ipv4_address] ttl = 10 } resource \"dns_a_record_set\" \"exercise_dns_name_record\" { zone = \"${var.dns_server_domain}.\" name = var.dns_server_name addresses = [hcloud_server.exercise_19.ipv4_address] ttl = 10 } resource \"dns_cname_record\" \"exercise_dns_alias_records\" { for_each = toset(var.dns_server_aliases) zone = \"${var.dns_server_domain}.\" name = each.key cname = \"${var.dns_server_name}.${var.dns_server_domain}.\" ttl = 10 } After running terraform apply now the DNS entries can be confirmed using dig . They will also be removed again when running terraform destroy : $ dig +noall +answer @ns1.hdm-stuttgart.cloud -y $HMAC -t AXFR g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 16 604800 86400 2419200 604800 g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 g2.sdi.hdm-stuttgart.cloud. 600 IN NS ns1.hdm-stuttgart.cloud. mail.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g2.sdi.hdm-stuttgart.cloud. workhorse.g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 www.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g2.sdi.hdm-stuttgart.cloud. g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 16 604800 86400 2419200 604800 $ terraform destroy ... $ dig +noall +answer @ns1.hdm-stuttgart.cloud -y $HMAC -t AXFR g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 20 604800 86400 2419200 604800 g2.sdi.hdm-stuttgart.cloud. 600 IN NS ns1.hdm-stuttgart.cloud. g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 20 604800 86400 2419200 604800","title":"Exercise 19"},{"location":"exercise19/#exercise-19-creating-dns-records","text":"Click here to view the solution in the repository. Currently the DNS configuration has only been done manually before in Exercise 18 . If the Terraform configuration is destroyed and then applied the IP may change making all the DNS entries invalid. It is much better to automatically create and update the DNS entries using Terraform. To do that it is ensured that there are no leftover DNS entries in the ns1.hdm-stuttgart.cloud nameserver: $ dig +noall +answer @ns1.hdm-stuttgart.cloud -y $HMAC -t AXFR g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 20 604800 86400 2419200 604800 g2.sdi.hdm-stuttgart.cloud. 600 IN NS ns1.hdm-stuttgart.cloud. g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 20 604800 86400 2419200 604800 To create different DNS records automatically in Terraform the dns provider can be used. It has to be configured with the address and secret key of the server to be able to create, update and delete DNS records. To hold the secret key, a variable has been created that will be injected using a non-versioned secrets.auto.tfvars file: variable \"dns_secret_key\" { description = \"Secret Key for the DNS nameserver\" type = string sensitive = true } provider \"dns\" { update { server = \"ns1.hdm-stuttgart.cloud\" key_name = \"g2.key.\" key_algorithm = \"hmac-sha512\" key_secret = var.dns_secret_key } } In this exercise two types of DNS records should be created: A and CNAME . A records have been discussed in Exercise 18 before. CNAME records are alias records: They point from one domain name to another domain name. The domain they are aliasing will then have to be looked up, following the chain until an IP address is found. The following records are created using Terraform: A base A record with the domain name g2.sdi.hdm-stuttgart.cloud pointing to the server IP A main A record with the name of the server appended to the domain (e.g. workhorse.g2.sdi.hdm-stuttgart.cloud ) also pointing to the server IP A set of CNAME aliases, all pointing to the main A record (e.g. mail.g2.sdi.hdm-stuttgart.cloud and www.g2.sdi.hdm-stuttgart.cloud ) To keep these records as modular as possible all the domains are specified using variables. The base domain, a main server name as well as a set of alias names (entries are automatically unique), which is validated to not contain the main server name: variable \"dns_server_domain\" { description = \"The base domain of the server\" type = string default = \"g2.sdi.hdm-stuttgart.cloud\" } variable \"dns_server_name\" { description = \"The name of the server\" type = string } variable \"dns_server_aliases\" { description = \"Alias names of the server\" type = set(string) default = [] validation { condition = !contains(var.dns_server_aliases, var.dns_server_name) error_message = \"Alias may not shadow the main name\" } } These variables are then used to create the above mentioned set of DNS records. The A records are straightforward. They just take in the domain and the IP address the record is supposed to be pointing to. The CNAME records take in the domain they are aliasing instead of the IP but due to there being a variable amount of aliases the for_each argument is used to create a Terraform loop which executes the resource once for every alias specified. The implicit attribute each.key is then used to get the individual name of each alias. resource \"dns_a_record_set\" \"exercise_dns_base_record\" { zone = \"${var.dns_server_domain}.\" addresses = [hcloud_server.exercise_19.ipv4_address] ttl = 10 } resource \"dns_a_record_set\" \"exercise_dns_name_record\" { zone = \"${var.dns_server_domain}.\" name = var.dns_server_name addresses = [hcloud_server.exercise_19.ipv4_address] ttl = 10 } resource \"dns_cname_record\" \"exercise_dns_alias_records\" { for_each = toset(var.dns_server_aliases) zone = \"${var.dns_server_domain}.\" name = each.key cname = \"${var.dns_server_name}.${var.dns_server_domain}.\" ttl = 10 } After running terraform apply now the DNS entries can be confirmed using dig . They will also be removed again when running terraform destroy : $ dig +noall +answer @ns1.hdm-stuttgart.cloud -y $HMAC -t AXFR g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 16 604800 86400 2419200 604800 g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 g2.sdi.hdm-stuttgart.cloud. 600 IN NS ns1.hdm-stuttgart.cloud. mail.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g2.sdi.hdm-stuttgart.cloud. workhorse.g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 www.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g2.sdi.hdm-stuttgart.cloud. g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 16 604800 86400 2419200 604800 $ terraform destroy ... $ dig +noall +answer @ns1.hdm-stuttgart.cloud -y $HMAC -t AXFR g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 20 604800 86400 2419200 604800 g2.sdi.hdm-stuttgart.cloud. 600 IN NS ns1.hdm-stuttgart.cloud. g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 20 604800 86400 2419200 604800","title":"Exercise 19: Creating DNS records"},{"location":"exercise20/","text":"Exercise 20: Creating a host with corresponding DNS entries Click here to view the solution in the repository. Since the creation of the known_hosts file and the ssh / scp wrappers is done in the ssh_wrapper module, the switch from IP based access to domain based access is simply a matter of exchanging the server_hostname input variable of the module. The module will then generate the right files in the right places with the correct server_hostname used: module \"ssh_wrapper\" { source = \"../modules/ssh_wrapper\" server_hostname = \"${var.dns_server_name}.${var.dns_server_domain}\" server_username = var.server_username server_public_key = tls_private_key.server_ssh_key.public_key_openssh output_dir = path.module } The resulting known_hosts and ssh.sh files have the adapted changes: $ cat gen/known_hosts workhorse.g2.sdi.hdm-stuttgart.cloud ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAID1YDz27SNLycwLWVC0HFiZAMQuY9Ja0i0WkHiOpm2rX $ cat bin/ssh.sh #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" devops@workhorse.g2.sdi.hdm-stuttgart.cloud \"$@\" Connecting still works without a problem: $ ./bin/ssh.sh Linux exercise-20 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. devops@exercise-20:~$","title":"Exercise 20"},{"location":"exercise20/#exercise-20-creating-a-host-with-corresponding-dns-entries","text":"Click here to view the solution in the repository. Since the creation of the known_hosts file and the ssh / scp wrappers is done in the ssh_wrapper module, the switch from IP based access to domain based access is simply a matter of exchanging the server_hostname input variable of the module. The module will then generate the right files in the right places with the correct server_hostname used: module \"ssh_wrapper\" { source = \"../modules/ssh_wrapper\" server_hostname = \"${var.dns_server_name}.${var.dns_server_domain}\" server_username = var.server_username server_public_key = tls_private_key.server_ssh_key.public_key_openssh output_dir = path.module } The resulting known_hosts and ssh.sh files have the adapted changes: $ cat gen/known_hosts workhorse.g2.sdi.hdm-stuttgart.cloud ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAID1YDz27SNLycwLWVC0HFiZAMQuY9Ja0i0WkHiOpm2rX $ cat bin/ssh.sh #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" devops@workhorse.g2.sdi.hdm-stuttgart.cloud \"$@\" Connecting still works without a problem: $ ./bin/ssh.sh Linux exercise-20 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. devops@exercise-20:~$","title":"Exercise 20: Creating a host with corresponding DNS entries"},{"location":"exercise21/","text":"Exercise 21: Creating a fixed number of servers Click here to view the solution in the repository. Note: This exercise is not used as base for the following exercises since it modifies and complicates some main aspects of the configuration. To be able to create a fixed number of servers, a lot of Terraform loops have to be used. Every resource, that belongs to one server will have to be looped and all the references between the resources have to be adjusted. Export the DNS module To prepare the process, the DNS registration process has been extracted into its own module: custom_dns . It takes in the arguments given before ( zone , base_name , alias and server_ip ) and builds the DNS records everytime it is called. This allows us to call the module in a loop for every server, drastically reducing the need for more loops. It is important to note that the module doesn't need the provider specification explicitly. As long as the provider is initialized in the root module it will be shared amongst called modules. It is even possible to specify separate provider initalizations and pass them individually to called modules. The created DNS module looks like the following (without variable declarations for brevity): terraform { required_providers { dns = { source = \"hashicorp/dns\" } } } resource \"dns_a_record_set\" \"exercise_dns_name_record\" { zone = \"${var.zone}.\" name = var.main_name addresses = [var.server_ip] ttl = 10 } resource \"dns_cname_record\" \"exercise_dns_alias_records\" { for_each = toset(var.alias) zone = \"${var.zone}.\" name = each.key cname = \"${var.main_name}.${var.zone}.\" ttl = 10 } Create Multiple Servers With that we can begin to create multiple servers. This is done using the Terraform count attribute which can be used to specify the number of that resource that should be created. If it is specified the count.index variable becomes implicitly available, storing the index of the current resource instance. This number can then be used by other referencing resources to select the right resource instance from the set: resource \"tls_private_key\" \"server_ssh_key\" { count = var.server_count algorithm = \"ED25519\" } resource \"local_file\" \"cloud_init\" { count = var.server_count filename = \"./server-${count.index}/gen/cloud_init.yml\" content = templatefile(\"./template/cloud_init.yml\", { default_user = var.server_username local_ssh_public = file(\"~/.ssh/id_ed25519.pub\") ssh_private = tls_private_key.server_ssh_key[count.index].private_key_openssh ssh_public = tls_private_key.server_ssh_key[count.index].public_key_openssh volume_mount = var.volume_mount volume_device = hcloud_volume.volume_exercise_20[count.index].linux_device }) } Another example is the creation of the main server and the calling of the modules: resource \"hcloud_server\" \"exercise_20\" { count = var.server_count name = \"exercise-20-${count.index}\" image = \"debian-12\" server_type = \"cpx11\" location = var.location user_data = local_file.cloud_init[count.index].content } resource \"hcloud_firewall_attachment\" \"exercise_fw_attachment\" { count = var.server_count firewall_id = hcloud_firewall.fw_exercise_20.id server_ids = [hcloud_server.exercise_20[count.index].id] lifecycle { replace_triggered_by = [hcloud_server.exercise_20[count.index], hcloud_firewall.fw_exercise_20] } } module \"ssh_wrapper\" { count = var.server_count source = \"../modules/ssh_wrapper\" server_hostname = \"${var.dns_server_name}-${count.index}.${var.dns_server_domain}\" server_username = var.server_username server_public_key = tls_private_key.server_ssh_key[count.index].public_key_openssh output_dir = \"${path.module}/server-${count.index}\" } module \"custom_dns\" { count = var.server_count source = \"../modules/custom_dns\" zone = \"g2.sdi.hdm-stuttgart.cloud\" main_name = \"${var.dns_server_name}-${count.index}\" server_ip = hcloud_server.exercise_20[count.index].ipv4_address alias = [ for alias in var.dns_server_aliases : \"${alias}-${count.index}\" ] } The ssh_wrapper module will deposit it's files based on the specified output file: server-0/bin , ... for the first server and server-1/bin , ... for the second one and so on. As is visible in the firewall_attachment resource, the firewall does not need to be created multiple times. It can simply be created once and be applied to multiple servers at the same time. Problems with the Approach Note that the resources are not inherently connected: A mismatch in arity of the resources would lead to reference issues. Since the variables are predefined the terraform plan / terraform apply commands would be able to derive the wrong references, but there is still a number of points of failure. One has to be extremely careful in which resources need to be looped and which doesnt - forgetting one can \"fail\" silently and only create issues later or don't even be noticed at all. If it was forgotten to duplicate the SSH keypair resource and the references, every server would have the same SSH private key and fingerprint - Terraform wouldn't notice this. To better encapsulate this behavior it would be sensible to extract the actual resources into modules like the DNS module. The module could then output values like the created server, volume and so on to allow access from the root module but this is beyond the scope of this exercise. The Working DNS Loop Due to the loop call of the DNS module we can observe multiple DNS entries for every server: One A record for each server pointing to it's IP and one CNAME record for every alias pointing to the main server. Because there are multiple server now the base server has been removed as a main A record since there isn't only one single server we could point g2.sdi.hdm-stuttgart.cloud to. Accessing the DNS records reveals that there is one record set for each server created: $ dig +noall +answer @ns1.hdm-stuttgart.cloud -y $HMAC -t AXFR g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 40 604800 86400 2419200 604800 g2.sdi.hdm-stuttgart.cloud. 600 IN NS ns1.hdm-stuttgart.cloud. mail-0.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME work-0.g2.sdi.hdm-stuttgart.cloud. mail-1.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME work-1.g2.sdi.hdm-stuttgart.cloud. work-0.g2.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 work-1.g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 www-0.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME work-0.g2.sdi.hdm-stuttgart.cloud. www-1.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME work-1.g2.sdi.hdm-stuttgart.cloud. g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 40 604800 86400 2419200 604800","title":"Exercise 21"},{"location":"exercise21/#exercise-21-creating-a-fixed-number-of-servers","text":"Click here to view the solution in the repository. Note: This exercise is not used as base for the following exercises since it modifies and complicates some main aspects of the configuration. To be able to create a fixed number of servers, a lot of Terraform loops have to be used. Every resource, that belongs to one server will have to be looped and all the references between the resources have to be adjusted.","title":"Exercise 21: Creating a fixed number of servers"},{"location":"exercise21/#export-the-dns-module","text":"To prepare the process, the DNS registration process has been extracted into its own module: custom_dns . It takes in the arguments given before ( zone , base_name , alias and server_ip ) and builds the DNS records everytime it is called. This allows us to call the module in a loop for every server, drastically reducing the need for more loops. It is important to note that the module doesn't need the provider specification explicitly. As long as the provider is initialized in the root module it will be shared amongst called modules. It is even possible to specify separate provider initalizations and pass them individually to called modules. The created DNS module looks like the following (without variable declarations for brevity): terraform { required_providers { dns = { source = \"hashicorp/dns\" } } } resource \"dns_a_record_set\" \"exercise_dns_name_record\" { zone = \"${var.zone}.\" name = var.main_name addresses = [var.server_ip] ttl = 10 } resource \"dns_cname_record\" \"exercise_dns_alias_records\" { for_each = toset(var.alias) zone = \"${var.zone}.\" name = each.key cname = \"${var.main_name}.${var.zone}.\" ttl = 10 }","title":"Export the DNS module"},{"location":"exercise21/#create-multiple-servers","text":"With that we can begin to create multiple servers. This is done using the Terraform count attribute which can be used to specify the number of that resource that should be created. If it is specified the count.index variable becomes implicitly available, storing the index of the current resource instance. This number can then be used by other referencing resources to select the right resource instance from the set: resource \"tls_private_key\" \"server_ssh_key\" { count = var.server_count algorithm = \"ED25519\" } resource \"local_file\" \"cloud_init\" { count = var.server_count filename = \"./server-${count.index}/gen/cloud_init.yml\" content = templatefile(\"./template/cloud_init.yml\", { default_user = var.server_username local_ssh_public = file(\"~/.ssh/id_ed25519.pub\") ssh_private = tls_private_key.server_ssh_key[count.index].private_key_openssh ssh_public = tls_private_key.server_ssh_key[count.index].public_key_openssh volume_mount = var.volume_mount volume_device = hcloud_volume.volume_exercise_20[count.index].linux_device }) } Another example is the creation of the main server and the calling of the modules: resource \"hcloud_server\" \"exercise_20\" { count = var.server_count name = \"exercise-20-${count.index}\" image = \"debian-12\" server_type = \"cpx11\" location = var.location user_data = local_file.cloud_init[count.index].content } resource \"hcloud_firewall_attachment\" \"exercise_fw_attachment\" { count = var.server_count firewall_id = hcloud_firewall.fw_exercise_20.id server_ids = [hcloud_server.exercise_20[count.index].id] lifecycle { replace_triggered_by = [hcloud_server.exercise_20[count.index], hcloud_firewall.fw_exercise_20] } } module \"ssh_wrapper\" { count = var.server_count source = \"../modules/ssh_wrapper\" server_hostname = \"${var.dns_server_name}-${count.index}.${var.dns_server_domain}\" server_username = var.server_username server_public_key = tls_private_key.server_ssh_key[count.index].public_key_openssh output_dir = \"${path.module}/server-${count.index}\" } module \"custom_dns\" { count = var.server_count source = \"../modules/custom_dns\" zone = \"g2.sdi.hdm-stuttgart.cloud\" main_name = \"${var.dns_server_name}-${count.index}\" server_ip = hcloud_server.exercise_20[count.index].ipv4_address alias = [ for alias in var.dns_server_aliases : \"${alias}-${count.index}\" ] } The ssh_wrapper module will deposit it's files based on the specified output file: server-0/bin , ... for the first server and server-1/bin , ... for the second one and so on. As is visible in the firewall_attachment resource, the firewall does not need to be created multiple times. It can simply be created once and be applied to multiple servers at the same time.","title":"Create Multiple Servers"},{"location":"exercise21/#problems-with-the-approach","text":"Note that the resources are not inherently connected: A mismatch in arity of the resources would lead to reference issues. Since the variables are predefined the terraform plan / terraform apply commands would be able to derive the wrong references, but there is still a number of points of failure. One has to be extremely careful in which resources need to be looped and which doesnt - forgetting one can \"fail\" silently and only create issues later or don't even be noticed at all. If it was forgotten to duplicate the SSH keypair resource and the references, every server would have the same SSH private key and fingerprint - Terraform wouldn't notice this. To better encapsulate this behavior it would be sensible to extract the actual resources into modules like the DNS module. The module could then output values like the created server, volume and so on to allow access from the root module but this is beyond the scope of this exercise.","title":"Problems with the Approach"},{"location":"exercise21/#the-working-dns-loop","text":"Due to the loop call of the DNS module we can observe multiple DNS entries for every server: One A record for each server pointing to it's IP and one CNAME record for every alias pointing to the main server. Because there are multiple server now the base server has been removed as a main A record since there isn't only one single server we could point g2.sdi.hdm-stuttgart.cloud to. Accessing the DNS records reveals that there is one record set for each server created: $ dig +noall +answer @ns1.hdm-stuttgart.cloud -y $HMAC -t AXFR g2.sdi.hdm-stuttgart.cloud g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 40 604800 86400 2419200 604800 g2.sdi.hdm-stuttgart.cloud. 600 IN NS ns1.hdm-stuttgart.cloud. mail-0.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME work-0.g2.sdi.hdm-stuttgart.cloud. mail-1.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME work-1.g2.sdi.hdm-stuttgart.cloud. work-0.g2.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 work-1.g2.sdi.hdm-stuttgart.cloud. 10 IN A 37.27.219.19 www-0.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME work-0.g2.sdi.hdm-stuttgart.cloud. www-1.g2.sdi.hdm-stuttgart.cloud. 10 IN CNAME work-1.g2.sdi.hdm-stuttgart.cloud. g2.sdi.hdm-stuttgart.cloud. 600 IN SOA ns1.hdm-stuttgart.cloud. goik\\@hdm-stuttgart.de. 40 604800 86400 2419200 604800","title":"The Working DNS Loop"},{"location":"exercise22/","text":"Exercise 22: Creating a web certificate Click here to view the solution in the repository. Note: The exercises 22 to 24 have been solved inside one configuration The acme provider is used in Terraform to request certificates from different certificate servers implementing the ACME protocol. To initialize the provider a server as well as a registration with that server is required: provider \"acme\" { server_url = \"https://acme-staging-v02.api.letsencrypt.org/directory\" # server_url = \"https://acme-v02.api.letsencrypt.org/directory\" } resource \"tls_private_key\" \"acme_account_private_key\" { algorithm = \"RSA\" } resource \"acme_registration\" \"registration\" { email_address = \"dw084@hdm-stuttgart.de\" account_key_pem = tls_private_key.acme_account_private_key.private_key_pem } The acme_certificate resource can then be used to request a certificate from that server. To do so the name of the domain that the certificate is registered to (in this case *.g2.sdi.hdm-stuttgart.cloud ) needs to be provided as well as any alternative names (in this case the zone domain g2.hdm-stuttgart.cloud ). The dns_challenge argument can be used to provide a DNS challenge in which the Let's encrypt server ensures that the necessary DNS entries are present in the DNS server before validating the certificate: resource \"acme_certificate\" \"wildcard_certificate\" { account_key_pem = acme_registration.registration.account_key_pem common_name = \"*.${var.dns_server_domain}\" subject_alternative_names = [var.dns_server_domain] dns_challenge { provider = \"rfc2136\" config = { RFC2136_NAMESERVER = \"ns1.sdi.hdm-stuttgart.cloud\" RFC2136_TSIG_ALGORITHM = \"hmac-sha512\" RFC2136_TSIG_KEY = \"g2.key.\" RFC2136_TSIG_SECRET = var.dns_secret_key } } } After the certificate has been created the values from it are used to save them to the gen folder in order to use them later (e.g. when applying them to a web server): resource \"local_file\" \"tls_private_key\" { filename = \"./gen/private.pem\" content = acme_certificate.wildcard_certificate.private_key_pem } resource \"local_file\" \"tls_certificate_key\" { filename = \"./gen/certificate.pem\" content = acme_certificate.wildcard_certificate.certificate_pem } dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ ./bin/ssh.sh Linux exercise-20 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. devops@exercise-20:~$ nano /etc/nginx/sites-available/default devops@exercise-20:~$ sudo nano /etc/nginx/sites-available/default devops@exercise-20:~$ devops@exercise-20:~$ sudo nano /etc/nginx/sites-available/default devops@exercise-20:~$ cat /etc/nginx/sites-available/default You should look at the following URL's in order to grasp a solid understanding of Nginx configuration files in order to fully unleash the power of Nginx. https://www.nginx.com/resources/wiki/start/ https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/ https://wiki.debian.org/Nginx/DirectoryStructure In most cases, administrators will remove this file from sites-enabled/ and leave it as reference inside of sites-available where it will continue to be updated by the nginx packaging team. This file will automatically load configuration files provided by other applications, such as Drupal or Wordpress. These applications will be made available underneath a path with that package name, such as /drupal8. Please see /usr/share/doc/nginx-doc/examples/ for more detailed examples. Default server configuration server { listen 80 default_server; listen [::]:80 default_server; # SSL configuration # listen 443 ssl default_server; listen [::]:443 ssl default_server; # # Note: You should disable gzip for SSL traffic. # See: https://bugs.debian.org/773332 # # Read up on ssl_ciphers to ensure a secure configuration. # See: https://bugs.debian.org/765782 # # Self signed certs generated by the ssl-cert package # Don't use them in a production server! # include snippets/snakeoil.conf; root /var/www/html; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } # pass PHP scripts to FastCGI server # #location ~ \\.php$ { # include snippets/fastcgi-php.conf; # # # With php-fpm (or other unix sockets): # fastcgi_pass unix:/run/php/php7.4-fpm.sock; # # With php-cgi (or other tcp sockets): # fastcgi_pass 127.0.0.1:9000; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } Virtual Host configuration for example.com You can move that to a different file under sites-available/ and symlink that to sites-enabled/ to enable it. server { listen 80; listen [::]:80; server_name example.com; root /var/www/example.com; index index.html; location / { try_files $uri $uri/ =404; } } devops@exercise-20:~$ sudo nano /etc/nginx/sites-available/default exit devops@exercise-20:~$ exit logout Connection to workhorse.g2.sdi.hdm-stuttgart.cloud closed. dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ terraform plan tls_private_key.server_ssh_key: Refreshing state... [id=001145e64304fd2ef3d3402f497542231dcc96ed] tls_private_key.acme_account_private_key: Refreshing state... [id=2c9bc6706ac1d22961dbbb31a3b2b3638ea4e294] hcloud_volume.volume_exercise_20: Refreshing state... [id=103002948] acme_registration.registration: Refreshing state... [id=https://acme-staging-v02.api.letsencrypt.org/acme/acct/217641194] hcloud_firewall.fw_exercise_20: Refreshing state... [id=2324824] module.ssh_wrapper.local_file.known_hosts: Refreshing state... [id=1315fc3b30c617cd2e3ce7b1fddb6e6b88006ede] local_file.tls_private_key: Refreshing state... [id=5c060afea8ae9ca731ee668e0ddc8d436b546d17] module.ssh_wrapper.local_file.ssh_bin: Refreshing state... [id=5ccf6ad6761693d2aa3b428cd2c14e518478a09f] module.ssh_wrapper.local_file.scp_bin: Refreshing state... [id=ecdfc0a87ce70109ab9805b8678e1d8197022680] dns_cname_record.exercise_dns_alias_records[\"www\"]: Refreshing state... [id=www.g2.sdi.hdm-stuttgart.cloud.] dns_cname_record.exercise_dns_alias_records[\"mail\"]: Refreshing state... [id=mail.g2.sdi.hdm-stuttgart.cloud.] acme_certificate.wildcard_certificate: Refreshing state... [id=7dc08b28-8dc7-8590-9bb3-2ccabdb791ba] local_file.tls_certificate_key: Refreshing state... [id=98628dcfa94d2a652b2b399b00fef103baf8bded] local_file.cloud_init: Refreshing state... [id=923a23b97f6dc5e555699ae9515c52886329bb01] hcloud_server.exercise_20: Refreshing state... [id=105771291] hcloud_volume_attachment.exercise_volume_attachment: Refreshing state... [id=103002948] hcloud_firewall_attachment.exercise_fw_attachment: Refreshing state... [id=2324824] dns_a_record_set.exercise_dns_name_record: Refreshing state... [id=workhorse.g2.sdi.hdm-stuttgart.cloud.] dns_a_record_set.exercise_dns_base_record: Refreshing state... [id=g2.sdi.hdm-stuttgart.cloud.] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place -/+ destroy and then create replacement Terraform will perform the following actions: # dns_a_record_set.exercise_dns_base_record will be updated in-place ~ resource \"dns_a_record_set\" \"exercise_dns_base_record\" { ~ addresses = [ - \"95.216.223.223\", ] -> (known after apply) id = \"g2.sdi.hdm-stuttgart.cloud.\" # (2 unchanged attributes hidden) } # dns_a_record_set.exercise_dns_name_record will be updated in-place ~ resource \"dns_a_record_set\" \"exercise_dns_name_record\" { ~ addresses = [ - \"95.216.223.223\", ] -> (known after apply) id = \"workhorse.g2.sdi.hdm-stuttgart.cloud.\" name = \"workhorse\" # (2 unchanged attributes hidden) } # hcloud_firewall_attachment.exercise_fw_attachment will be replaced due to changes in replace_triggered_by -/+ resource \"hcloud_firewall_attachment\" \"exercise_fw_attachment\" { ~ id = \"2324824\" -> (known after apply) - label_selectors = [] -> null ~ server_ids = [ - 105771291, ] -> (known after apply) # (1 unchanged attribute hidden) } # hcloud_server.exercise_20 must be replaced -/+ resource \"hcloud_server\" \"exercise_20\" { + backup_window = (known after apply) ~ datacenter = \"hel1-dc2\" -> (known after apply) ~ firewall_ids = [ - 2324824, ] -> (known after apply) ~ id = \"105771291\" -> (known after apply) ~ ipv4_address = \"95.216.223.223\" -> (known after apply) ~ ipv6_address = \"2a01:4f9:c013:e1a8::1\" -> (known after apply) ~ ipv6_network = \"2a01:4f9:c013:e1a8::/64\" -> (known after apply) - labels = {} -> null name = \"exercise-20\" - placement_group_id = 0 -> null ~ primary_disk_size = 40 -> (known after apply) ~ status = \"running\" -> (known after apply) ~ user_data = (sensitive value) # forces replacement # (10 unchanged attributes hidden) } # hcloud_volume_attachment.exercise_volume_attachment will be replaced due to changes in replace_triggered_by -/+ resource \"hcloud_volume_attachment\" \"exercise_volume_attachment\" { ~ id = \"103002948\" -> (known after apply) ~ server_id = 105771291 -> (known after apply) # forces replacement # (2 unchanged attributes hidden) } # local_file.cloud_init must be replaced -/+ resource \"local_file\" \"cloud_init\" { ~ content = (sensitive value) # forces replacement ~ content_base64sha256 = \"yH7vuZ6QXui5Arp74m5Cd+UH6olTwf1FTw7+1iU0iC8=\" -> (known after apply) ~ content_base64sha512 = \"7GRH9TjHKRwYEPH5eEjC8saDFnVGGGIo7HVqjxECaFSEZUX3sWGLpSBIvxPbYd2aTXIrRSMferzdz/syClxYSQ==\" -> (known after apply) ~ content_md5 = \"0ff4cc438028c46636c28540146f5df5\" -> (known after apply) ~ content_sha1 = \"923a23b97f6dc5e555699ae9515c52886329bb01\" -> (known after apply) ~ content_sha256 = \"c87eefb99e905ee8b902ba7be26e4277e507ea8953c1fd454f0efed62534882f\" -> (known after apply) ~ content_sha512 = \"ec6447f538c7291c1810f1f97848c2f2c683167546186228ec756a8f11026854846545f7b1618ba52048bf13db61dd9a4d722b45231f7abcddcffb320a5c5849\" -> (known after apply) ~ id = \"923a23b97f6dc5e555699ae9515c52886329bb01\" -> (known after apply) # (3 unchanged attributes hidden) } Plan: 4 to add, 2 to change, 4 to destroy. Changes to Outputs: ~ server_datacenter = \"hel1-dc2\" -> (known after apply) ~ server_ip = \"95.216.223.223\" -> (known after apply) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ terraform apply dns_cname_record.exercise_dns_alias_records[\"www\"]: Refreshing state... [id=www.g2.sdi.hdm-stuttgart.cloud.] dns_cname_record.exercise_dns_alias_records[\"mail\"]: Refreshing state... [id=mail.g2.sdi.hdm-stuttgart.cloud.] hcloud_volume.volume_exercise_20: Refreshing state... [id=103002948] hcloud_firewall.fw_exercise_20: Refreshing state... [id=2324824] tls_private_key.acme_account_private_key: Refreshing state... [id=2c9bc6706ac1d22961dbbb31a3b2b3638ea4e294] tls_private_key.server_ssh_key: Refreshing state... [id=001145e64304fd2ef3d3402f497542231dcc96ed] acme_registration.registration: Refreshing state... [id=https://acme-staging-v02.api.letsencrypt.org/acme/acct/217641194] local_file.tls_private_key: Refreshing state... [id=5c060afea8ae9ca731ee668e0ddc8d436b546d17] module.ssh_wrapper.local_file.known_hosts: Refreshing state... [id=1315fc3b30c617cd2e3ce7b1fddb6e6b88006ede] module.ssh_wrapper.local_file.scp_bin: Refreshing state... [id=ecdfc0a87ce70109ab9805b8678e1d8197022680] module.ssh_wrapper.local_file.ssh_bin: Refreshing state... [id=5ccf6ad6761693d2aa3b428cd2c14e518478a09f] acme_certificate.wildcard_certificate: Refreshing state... [id=7dc08b28-8dc7-8590-9bb3-2ccabdb791ba] local_file.tls_certificate_key: Refreshing state... [id=98628dcfa94d2a652b2b399b00fef103baf8bded] local_file.cloud_init: Refreshing state... [id=923a23b97f6dc5e555699ae9515c52886329bb01] hcloud_server.exercise_20: Refreshing state... [id=105771291] dns_a_record_set.exercise_dns_name_record: Refreshing state... [id=workhorse.g2.sdi.hdm-stuttgart.cloud.] dns_a_record_set.exercise_dns_base_record: Refreshing state... [id=g2.sdi.hdm-stuttgart.cloud.] hcloud_volume_attachment.exercise_volume_attachment: Refreshing state... [id=103002948] hcloud_firewall_attachment.exercise_fw_attachment: Refreshing state... [id=2324824] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place -/+ destroy and then create replacement Terraform will perform the following actions: # dns_a_record_set.exercise_dns_base_record will be updated in-place ~ resource \"dns_a_record_set\" \"exercise_dns_base_record\" { ~ addresses = [ - \"95.216.223.223\", ] -> (known after apply) id = \"g2.sdi.hdm-stuttgart.cloud.\" # (2 unchanged attributes hidden) } # dns_a_record_set.exercise_dns_name_record will be updated in-place ~ resource \"dns_a_record_set\" \"exercise_dns_name_record\" { ~ addresses = [ - \"95.216.223.223\", ] -> (known after apply) id = \"workhorse.g2.sdi.hdm-stuttgart.cloud.\" name = \"workhorse\" # (2 unchanged attributes hidden) } # hcloud_firewall_attachment.exercise_fw_attachment will be replaced due to changes in replace_triggered_by -/+ resource \"hcloud_firewall_attachment\" \"exercise_fw_attachment\" { ~ id = \"2324824\" -> (known after apply) - label_selectors = [] -> null ~ server_ids = [ - 105771291, ] -> (known after apply) # (1 unchanged attribute hidden) } # hcloud_server.exercise_20 must be replaced -/+ resource \"hcloud_server\" \"exercise_20\" { + backup_window = (known after apply) ~ datacenter = \"hel1-dc2\" -> (known after apply) ~ firewall_ids = [ - 2324824, ] -> (known after apply) ~ id = \"105771291\" -> (known after apply) ~ ipv4_address = \"95.216.223.223\" -> (known after apply) ~ ipv6_address = \"2a01:4f9:c013:e1a8::1\" -> (known after apply) ~ ipv6_network = \"2a01:4f9:c013:e1a8::/64\" -> (known after apply) - labels = {} -> null name = \"exercise-20\" - placement_group_id = 0 -> null ~ primary_disk_size = 40 -> (known after apply) ~ status = \"running\" -> (known after apply) ~ user_data = (sensitive value) # forces replacement # (10 unchanged attributes hidden) } # hcloud_volume_attachment.exercise_volume_attachment will be replaced due to changes in replace_triggered_by -/+ resource \"hcloud_volume_attachment\" \"exercise_volume_attachment\" { ~ id = \"103002948\" -> (known after apply) ~ server_id = 105771291 -> (known after apply) # forces replacement # (2 unchanged attributes hidden) } # local_file.cloud_init must be replaced -/+ resource \"local_file\" \"cloud_init\" { ~ content = (sensitive value) # forces replacement ~ content_base64sha256 = \"yH7vuZ6QXui5Arp74m5Cd+UH6olTwf1FTw7+1iU0iC8=\" -> (known after apply) ~ content_base64sha512 = \"7GRH9TjHKRwYEPH5eEjC8saDFnVGGGIo7HVqjxECaFSEZUX3sWGLpSBIvxPbYd2aTXIrRSMferzdz/syClxYSQ==\" -> (known after apply) ~ content_md5 = \"0ff4cc438028c46636c28540146f5df5\" -> (known after apply) ~ content_sha1 = \"923a23b97f6dc5e555699ae9515c52886329bb01\" -> (known after apply) ~ content_sha256 = \"c87eefb99e905ee8b902ba7be26e4277e507ea8953c1fd454f0efed62534882f\" -> (known after apply) ~ content_sha512 = \"ec6447f538c7291c1810f1f97848c2f2c683167546186228ec756a8f11026854846545f7b1618ba52048bf13db61dd9a4d722b45231f7abcddcffb320a5c5849\" -> (known after apply) ~ id = \"923a23b97f6dc5e555699ae9515c52886329bb01\" -> (known after apply) # (3 unchanged attributes hidden) } Plan: 4 to add, 2 to change, 4 to destroy. Changes to Outputs: ~ server_datacenter = \"hel1-dc2\" -> (known after apply) ~ server_ip = \"95.216.223.223\" -> (known after apply) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes hcloud_volume_attachment.exercise_volume_attachment: Destroying... [id=103002948] hcloud_firewall_attachment.exercise_fw_attachment: Destroying... [id=2324824] hcloud_firewall_attachment.exercise_fw_attachment: Destruction complete after 2s hcloud_volume_attachment.exercise_volume_attachment: Destruction complete after 4s hcloud_server.exercise_20: Destroying... [id=105771291] hcloud_server.exercise_20: Destruction complete after 8s local_file.cloud_init: Destroying... [id=923a23b97f6dc5e555699ae9515c52886329bb01] local_file.cloud_init: Destruction complete after 0s local_file.cloud_init: Creating... local_file.cloud_init: Creation complete after 0s [id=6bd71fdc395ac3d3dd94c75297cae1e359bb58ad] hcloud_server.exercise_20: Creating... hcloud_server.exercise_20: Still creating... [00m10s elapsed] hcloud_server.exercise_20: Creation complete after 12s [id=105772030] hcloud_volume_attachment.exercise_volume_attachment: Creating... hcloud_firewall_attachment.exercise_fw_attachment: Creating... hcloud_firewall_attachment.exercise_fw_attachment: Creation complete after 2s [id=2324824] hcloud_volume_attachment.exercise_volume_attachment: Creation complete after 8s [id=103002948] Apply complete! Resources: 4 added, 0 changed, 4 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" volume_device = \"/dev/disk/by-id/scsi-0HC_Volume_103002948\" dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ $ ./bin/ssh.sh Linux exercise-20 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. devops@exercise-20:~$ openssl rsa -in /etc/nginx/ssl/private.pem -noout -modulus | openssl md5 MD5(stdin)= 9b486c6d6d539a37e3b152cfbfcd1b9b devops@exercise-20:~$ openssl x509 -in /etc/nginx/ssl/.pem -noout -modulus | openssl md5 certificate.pem private.pem devops@exercise-20:~$ openssl x509 -in /etc/nginx/ssl/.pem -noout -modulus | openssl md5 certificate.pem private.pem devops@exercise-20:~$ openssl x509 -in /etc/nginx/ssl/certificate.pem -noout -modulus | openssl md5 MD5(stdin)= 9b486c6d6d539a37e3b152cfbfcd1b9b devops@exercise-20:~$ sudo nano /etc/nginx/sites-available/default devops@exercise-20:~$ sudo nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful devops@exercise-20:~$ systemctl restart nginx Failed to execute /usr/bin/pkttyagent: No such file or directory Failed to restart nginx.service: Access denied See system logs and 'systemctl status nginx.service' for details. devops@exercise-20:~$ sudo systemctl restart nginx devops@exercise-20:~$ sudo systemctl status nginx \u25cf nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; preset: enabled) Active: active (running) since Fri 2025-08-01 14:20:14 UTC; 6s ago Docs: man:nginx(8) Process: 2517 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Process: 2518 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Main PID: 2519 (nginx) Tasks: 3 (limit: 2250) Memory: 2.7M CPU: 15ms CGroup: /system.slice/nginx.service \u251c\u25002519 \"nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\" \u251c\u25002520 \"nginx: worker process\" \u2514\u25002521 \"nginx: worker process\" Aug 01 14:20:14 exercise-20 systemd[1]: Starting nginx.service - A high performance web server and a reverse proxy server... Aug 01 14:20:14 exercise-20 systemd[1]: Started nginx.service - A high performance web server and a reverse proxy server. devops@exercise-20:~$ exit logout Connection to workhorse.g2.sdi.hdm-stuttgart.cloud closed. dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ ./bin/ssh.sh Linux exercise-20 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. Last login: Fri Aug 1 14:16:22 2025 from 91.16.131.250 devops@exercise-20:~$ cat / bin/ dev/ home/ initrd.img.old lib64/ media/ opt/ root/ sbin/ sys/ usr/ vmlinuz volume01/ boot/ etc/ initrd.img lib/ lost+found/ mnt/ proc/ run/ srv/ tmp/ var/ vmlinuz.old devops@exercise-20:~$ cat / bin/ dev/ home/ initrd.img.old lib64/ media/ opt/ root/ sbin/ sys/ usr/ vmlinuz volume01/ boot/ etc/ initrd.img lib/ lost+found/ mnt/ proc/ run/ srv/ tmp/ var/ vmlinuz.old devops@exercise-20:~$ cat /etc/nginx/sites-available/default You should look at the following URL's in order to grasp a solid understanding of Nginx configuration files in order to fully unleash the power of Nginx. https://www.nginx.com/resources/wiki/start/ https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/ https://wiki.debian.org/Nginx/DirectoryStructure In most cases, administrators will remove this file from sites-enabled/ and leave it as reference inside of sites-available where it will continue to be updated by the nginx packaging team. This file will automatically load configuration files provided by other applications, such as Drupal or Wordpress. These applications will be made available underneath a path with that package name, such as /drupal8. Please see /usr/share/doc/nginx-doc/examples/ for more detailed examples. Default server configuration server { listen 80 default_server; listen [::]:80 default_server; # SSL configuration # listen 443 ssl default_server; listen [::]:443 ssl default_server; ssl_certificate /etc/nginx/ssl/certificate.pem; ssl_certificate_key /etc/nginx/ssl/private.pem; # # Note: You should disable gzip for SSL traffic. # See: https://bugs.debian.org/773332 # # Read up on ssl_ciphers to ensure a secure configuration. # See: https://bugs.debian.org/765782 # # Self signed certs generated by the ssl-cert package # Don't use them in a production server! # # include snippets/snakeoil.conf; root /var/www/html; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } # pass PHP scripts to FastCGI server # #location ~ \\.php$ { # include snippets/fastcgi-php.conf; # # # With php-fpm (or other unix sockets): # fastcgi_pass unix:/run/php/php7.4-fpm.sock; # # With php-cgi (or other tcp sockets): # fastcgi_pass 127.0.0.1:9000; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } Virtual Host configuration for example.com You can move that to a different file under sites-available/ and symlink that to sites-enabled/ to enable it. server { listen 80; listen [::]:80; server_name example.com; root /var/www/example.com; index index.html; location / { try_files $uri $uri/ =404; } } devops@exercise-20:~$","title":"Exercise 22"},{"location":"exercise22/#exercise-22-creating-a-web-certificate","text":"Click here to view the solution in the repository. Note: The exercises 22 to 24 have been solved inside one configuration The acme provider is used in Terraform to request certificates from different certificate servers implementing the ACME protocol. To initialize the provider a server as well as a registration with that server is required: provider \"acme\" { server_url = \"https://acme-staging-v02.api.letsencrypt.org/directory\" # server_url = \"https://acme-v02.api.letsencrypt.org/directory\" } resource \"tls_private_key\" \"acme_account_private_key\" { algorithm = \"RSA\" } resource \"acme_registration\" \"registration\" { email_address = \"dw084@hdm-stuttgart.de\" account_key_pem = tls_private_key.acme_account_private_key.private_key_pem } The acme_certificate resource can then be used to request a certificate from that server. To do so the name of the domain that the certificate is registered to (in this case *.g2.sdi.hdm-stuttgart.cloud ) needs to be provided as well as any alternative names (in this case the zone domain g2.hdm-stuttgart.cloud ). The dns_challenge argument can be used to provide a DNS challenge in which the Let's encrypt server ensures that the necessary DNS entries are present in the DNS server before validating the certificate: resource \"acme_certificate\" \"wildcard_certificate\" { account_key_pem = acme_registration.registration.account_key_pem common_name = \"*.${var.dns_server_domain}\" subject_alternative_names = [var.dns_server_domain] dns_challenge { provider = \"rfc2136\" config = { RFC2136_NAMESERVER = \"ns1.sdi.hdm-stuttgart.cloud\" RFC2136_TSIG_ALGORITHM = \"hmac-sha512\" RFC2136_TSIG_KEY = \"g2.key.\" RFC2136_TSIG_SECRET = var.dns_secret_key } } } After the certificate has been created the values from it are used to save them to the gen folder in order to use them later (e.g. when applying them to a web server): resource \"local_file\" \"tls_private_key\" { filename = \"./gen/private.pem\" content = acme_certificate.wildcard_certificate.private_key_pem } resource \"local_file\" \"tls_certificate_key\" { filename = \"./gen/certificate.pem\" content = acme_certificate.wildcard_certificate.certificate_pem } dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ ./bin/ssh.sh Linux exercise-20 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. devops@exercise-20:~$ nano /etc/nginx/sites-available/default devops@exercise-20:~$ sudo nano /etc/nginx/sites-available/default devops@exercise-20:~$ devops@exercise-20:~$ sudo nano /etc/nginx/sites-available/default devops@exercise-20:~$ cat /etc/nginx/sites-available/default","title":"Exercise 22: Creating a web certificate"},{"location":"exercise22/#_1","text":"","title":""},{"location":"exercise22/#you-should-look-at-the-following-urls-in-order-to-grasp-a-solid-understanding","text":"","title":"You should look at the following URL's in order to grasp a solid understanding"},{"location":"exercise22/#of-nginx-configuration-files-in-order-to-fully-unleash-the-power-of-nginx","text":"","title":"of Nginx configuration files in order to fully unleash the power of Nginx."},{"location":"exercise22/#httpswwwnginxcomresourceswikistart","text":"","title":"https://www.nginx.com/resources/wiki/start/"},{"location":"exercise22/#httpswwwnginxcomresourceswikistarttopicstutorialsconfig_pitfalls","text":"","title":"https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/"},{"location":"exercise22/#httpswikidebianorgnginxdirectorystructure","text":"","title":"https://wiki.debian.org/Nginx/DirectoryStructure"},{"location":"exercise22/#_2","text":"","title":""},{"location":"exercise22/#in-most-cases-administrators-will-remove-this-file-from-sites-enabled-and","text":"","title":"In most cases, administrators will remove this file from sites-enabled/ and"},{"location":"exercise22/#leave-it-as-reference-inside-of-sites-available-where-it-will-continue-to-be","text":"","title":"leave it as reference inside of sites-available where it will continue to be"},{"location":"exercise22/#updated-by-the-nginx-packaging-team","text":"","title":"updated by the nginx packaging team."},{"location":"exercise22/#_3","text":"","title":""},{"location":"exercise22/#this-file-will-automatically-load-configuration-files-provided-by-other","text":"","title":"This file will automatically load configuration files provided by other"},{"location":"exercise22/#applications-such-as-drupal-or-wordpress-these-applications-will-be-made","text":"","title":"applications, such as Drupal or Wordpress. These applications will be made"},{"location":"exercise22/#available-underneath-a-path-with-that-package-name-such-as-drupal8","text":"","title":"available underneath a path with that package name, such as /drupal8."},{"location":"exercise22/#_4","text":"","title":""},{"location":"exercise22/#please-see-usrsharedocnginx-docexamples-for-more-detailed-examples","text":"","title":"Please see /usr/share/doc/nginx-doc/examples/ for more detailed examples."},{"location":"exercise22/#_5","text":"","title":""},{"location":"exercise22/#default-server-configuration","text":"","title":"Default server configuration"},{"location":"exercise22/#_6","text":"server { listen 80 default_server; listen [::]:80 default_server; # SSL configuration # listen 443 ssl default_server; listen [::]:443 ssl default_server; # # Note: You should disable gzip for SSL traffic. # See: https://bugs.debian.org/773332 # # Read up on ssl_ciphers to ensure a secure configuration. # See: https://bugs.debian.org/765782 # # Self signed certs generated by the ssl-cert package # Don't use them in a production server! # include snippets/snakeoil.conf; root /var/www/html; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } # pass PHP scripts to FastCGI server # #location ~ \\.php$ { # include snippets/fastcgi-php.conf; # # # With php-fpm (or other unix sockets): # fastcgi_pass unix:/run/php/php7.4-fpm.sock; # # With php-cgi (or other tcp sockets): # fastcgi_pass 127.0.0.1:9000; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} }","title":""},{"location":"exercise22/#virtual-host-configuration-for-examplecom","text":"","title":"Virtual Host configuration for example.com"},{"location":"exercise22/#_7","text":"","title":""},{"location":"exercise22/#you-can-move-that-to-a-different-file-under-sites-available-and-symlink-that","text":"","title":"You can move that to a different file under sites-available/ and symlink that"},{"location":"exercise22/#to-sites-enabled-to-enable-it","text":"","title":"to sites-enabled/ to enable it."},{"location":"exercise22/#_8","text":"","title":""},{"location":"exercise22/#server","text":"","title":"server {"},{"location":"exercise22/#listen-80","text":"","title":"listen 80;"},{"location":"exercise22/#listen-80_1","text":"","title":"listen [::]:80;"},{"location":"exercise22/#_9","text":"","title":""},{"location":"exercise22/#server_name-examplecom","text":"","title":"server_name example.com;"},{"location":"exercise22/#_10","text":"","title":""},{"location":"exercise22/#root-varwwwexamplecom","text":"","title":"root /var/www/example.com;"},{"location":"exercise22/#index-indexhtml","text":"","title":"index index.html;"},{"location":"exercise22/#_11","text":"","title":""},{"location":"exercise22/#location","text":"","title":"location / {"},{"location":"exercise22/#try_files-uri-uri-404","text":"","title":"try_files $uri $uri/ =404;"},{"location":"exercise22/#_12","text":"","title":"}"},{"location":"exercise22/#_13","text":"devops@exercise-20:~$ sudo nano /etc/nginx/sites-available/default exit devops@exercise-20:~$ exit logout Connection to workhorse.g2.sdi.hdm-stuttgart.cloud closed. dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ terraform plan tls_private_key.server_ssh_key: Refreshing state... [id=001145e64304fd2ef3d3402f497542231dcc96ed] tls_private_key.acme_account_private_key: Refreshing state... [id=2c9bc6706ac1d22961dbbb31a3b2b3638ea4e294] hcloud_volume.volume_exercise_20: Refreshing state... [id=103002948] acme_registration.registration: Refreshing state... [id=https://acme-staging-v02.api.letsencrypt.org/acme/acct/217641194] hcloud_firewall.fw_exercise_20: Refreshing state... [id=2324824] module.ssh_wrapper.local_file.known_hosts: Refreshing state... [id=1315fc3b30c617cd2e3ce7b1fddb6e6b88006ede] local_file.tls_private_key: Refreshing state... [id=5c060afea8ae9ca731ee668e0ddc8d436b546d17] module.ssh_wrapper.local_file.ssh_bin: Refreshing state... [id=5ccf6ad6761693d2aa3b428cd2c14e518478a09f] module.ssh_wrapper.local_file.scp_bin: Refreshing state... [id=ecdfc0a87ce70109ab9805b8678e1d8197022680] dns_cname_record.exercise_dns_alias_records[\"www\"]: Refreshing state... [id=www.g2.sdi.hdm-stuttgart.cloud.] dns_cname_record.exercise_dns_alias_records[\"mail\"]: Refreshing state... [id=mail.g2.sdi.hdm-stuttgart.cloud.] acme_certificate.wildcard_certificate: Refreshing state... [id=7dc08b28-8dc7-8590-9bb3-2ccabdb791ba] local_file.tls_certificate_key: Refreshing state... [id=98628dcfa94d2a652b2b399b00fef103baf8bded] local_file.cloud_init: Refreshing state... [id=923a23b97f6dc5e555699ae9515c52886329bb01] hcloud_server.exercise_20: Refreshing state... [id=105771291] hcloud_volume_attachment.exercise_volume_attachment: Refreshing state... [id=103002948] hcloud_firewall_attachment.exercise_fw_attachment: Refreshing state... [id=2324824] dns_a_record_set.exercise_dns_name_record: Refreshing state... [id=workhorse.g2.sdi.hdm-stuttgart.cloud.] dns_a_record_set.exercise_dns_base_record: Refreshing state... [id=g2.sdi.hdm-stuttgart.cloud.] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place -/+ destroy and then create replacement Terraform will perform the following actions: # dns_a_record_set.exercise_dns_base_record will be updated in-place ~ resource \"dns_a_record_set\" \"exercise_dns_base_record\" { ~ addresses = [ - \"95.216.223.223\", ] -> (known after apply) id = \"g2.sdi.hdm-stuttgart.cloud.\" # (2 unchanged attributes hidden) } # dns_a_record_set.exercise_dns_name_record will be updated in-place ~ resource \"dns_a_record_set\" \"exercise_dns_name_record\" { ~ addresses = [ - \"95.216.223.223\", ] -> (known after apply) id = \"workhorse.g2.sdi.hdm-stuttgart.cloud.\" name = \"workhorse\" # (2 unchanged attributes hidden) } # hcloud_firewall_attachment.exercise_fw_attachment will be replaced due to changes in replace_triggered_by -/+ resource \"hcloud_firewall_attachment\" \"exercise_fw_attachment\" { ~ id = \"2324824\" -> (known after apply) - label_selectors = [] -> null ~ server_ids = [ - 105771291, ] -> (known after apply) # (1 unchanged attribute hidden) } # hcloud_server.exercise_20 must be replaced -/+ resource \"hcloud_server\" \"exercise_20\" { + backup_window = (known after apply) ~ datacenter = \"hel1-dc2\" -> (known after apply) ~ firewall_ids = [ - 2324824, ] -> (known after apply) ~ id = \"105771291\" -> (known after apply) ~ ipv4_address = \"95.216.223.223\" -> (known after apply) ~ ipv6_address = \"2a01:4f9:c013:e1a8::1\" -> (known after apply) ~ ipv6_network = \"2a01:4f9:c013:e1a8::/64\" -> (known after apply) - labels = {} -> null name = \"exercise-20\" - placement_group_id = 0 -> null ~ primary_disk_size = 40 -> (known after apply) ~ status = \"running\" -> (known after apply) ~ user_data = (sensitive value) # forces replacement # (10 unchanged attributes hidden) } # hcloud_volume_attachment.exercise_volume_attachment will be replaced due to changes in replace_triggered_by -/+ resource \"hcloud_volume_attachment\" \"exercise_volume_attachment\" { ~ id = \"103002948\" -> (known after apply) ~ server_id = 105771291 -> (known after apply) # forces replacement # (2 unchanged attributes hidden) } # local_file.cloud_init must be replaced -/+ resource \"local_file\" \"cloud_init\" { ~ content = (sensitive value) # forces replacement ~ content_base64sha256 = \"yH7vuZ6QXui5Arp74m5Cd+UH6olTwf1FTw7+1iU0iC8=\" -> (known after apply) ~ content_base64sha512 = \"7GRH9TjHKRwYEPH5eEjC8saDFnVGGGIo7HVqjxECaFSEZUX3sWGLpSBIvxPbYd2aTXIrRSMferzdz/syClxYSQ==\" -> (known after apply) ~ content_md5 = \"0ff4cc438028c46636c28540146f5df5\" -> (known after apply) ~ content_sha1 = \"923a23b97f6dc5e555699ae9515c52886329bb01\" -> (known after apply) ~ content_sha256 = \"c87eefb99e905ee8b902ba7be26e4277e507ea8953c1fd454f0efed62534882f\" -> (known after apply) ~ content_sha512 = \"ec6447f538c7291c1810f1f97848c2f2c683167546186228ec756a8f11026854846545f7b1618ba52048bf13db61dd9a4d722b45231f7abcddcffb320a5c5849\" -> (known after apply) ~ id = \"923a23b97f6dc5e555699ae9515c52886329bb01\" -> (known after apply) # (3 unchanged attributes hidden) } Plan: 4 to add, 2 to change, 4 to destroy. Changes to Outputs: ~ server_datacenter = \"hel1-dc2\" -> (known after apply) ~ server_ip = \"95.216.223.223\" -> (known after apply) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ terraform apply dns_cname_record.exercise_dns_alias_records[\"www\"]: Refreshing state... [id=www.g2.sdi.hdm-stuttgart.cloud.] dns_cname_record.exercise_dns_alias_records[\"mail\"]: Refreshing state... [id=mail.g2.sdi.hdm-stuttgart.cloud.] hcloud_volume.volume_exercise_20: Refreshing state... [id=103002948] hcloud_firewall.fw_exercise_20: Refreshing state... [id=2324824] tls_private_key.acme_account_private_key: Refreshing state... [id=2c9bc6706ac1d22961dbbb31a3b2b3638ea4e294] tls_private_key.server_ssh_key: Refreshing state... [id=001145e64304fd2ef3d3402f497542231dcc96ed] acme_registration.registration: Refreshing state... [id=https://acme-staging-v02.api.letsencrypt.org/acme/acct/217641194] local_file.tls_private_key: Refreshing state... [id=5c060afea8ae9ca731ee668e0ddc8d436b546d17] module.ssh_wrapper.local_file.known_hosts: Refreshing state... [id=1315fc3b30c617cd2e3ce7b1fddb6e6b88006ede] module.ssh_wrapper.local_file.scp_bin: Refreshing state... [id=ecdfc0a87ce70109ab9805b8678e1d8197022680] module.ssh_wrapper.local_file.ssh_bin: Refreshing state... [id=5ccf6ad6761693d2aa3b428cd2c14e518478a09f] acme_certificate.wildcard_certificate: Refreshing state... [id=7dc08b28-8dc7-8590-9bb3-2ccabdb791ba] local_file.tls_certificate_key: Refreshing state... [id=98628dcfa94d2a652b2b399b00fef103baf8bded] local_file.cloud_init: Refreshing state... [id=923a23b97f6dc5e555699ae9515c52886329bb01] hcloud_server.exercise_20: Refreshing state... [id=105771291] dns_a_record_set.exercise_dns_name_record: Refreshing state... [id=workhorse.g2.sdi.hdm-stuttgart.cloud.] dns_a_record_set.exercise_dns_base_record: Refreshing state... [id=g2.sdi.hdm-stuttgart.cloud.] hcloud_volume_attachment.exercise_volume_attachment: Refreshing state... [id=103002948] hcloud_firewall_attachment.exercise_fw_attachment: Refreshing state... [id=2324824] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place -/+ destroy and then create replacement Terraform will perform the following actions: # dns_a_record_set.exercise_dns_base_record will be updated in-place ~ resource \"dns_a_record_set\" \"exercise_dns_base_record\" { ~ addresses = [ - \"95.216.223.223\", ] -> (known after apply) id = \"g2.sdi.hdm-stuttgart.cloud.\" # (2 unchanged attributes hidden) } # dns_a_record_set.exercise_dns_name_record will be updated in-place ~ resource \"dns_a_record_set\" \"exercise_dns_name_record\" { ~ addresses = [ - \"95.216.223.223\", ] -> (known after apply) id = \"workhorse.g2.sdi.hdm-stuttgart.cloud.\" name = \"workhorse\" # (2 unchanged attributes hidden) } # hcloud_firewall_attachment.exercise_fw_attachment will be replaced due to changes in replace_triggered_by -/+ resource \"hcloud_firewall_attachment\" \"exercise_fw_attachment\" { ~ id = \"2324824\" -> (known after apply) - label_selectors = [] -> null ~ server_ids = [ - 105771291, ] -> (known after apply) # (1 unchanged attribute hidden) } # hcloud_server.exercise_20 must be replaced -/+ resource \"hcloud_server\" \"exercise_20\" { + backup_window = (known after apply) ~ datacenter = \"hel1-dc2\" -> (known after apply) ~ firewall_ids = [ - 2324824, ] -> (known after apply) ~ id = \"105771291\" -> (known after apply) ~ ipv4_address = \"95.216.223.223\" -> (known after apply) ~ ipv6_address = \"2a01:4f9:c013:e1a8::1\" -> (known after apply) ~ ipv6_network = \"2a01:4f9:c013:e1a8::/64\" -> (known after apply) - labels = {} -> null name = \"exercise-20\" - placement_group_id = 0 -> null ~ primary_disk_size = 40 -> (known after apply) ~ status = \"running\" -> (known after apply) ~ user_data = (sensitive value) # forces replacement # (10 unchanged attributes hidden) } # hcloud_volume_attachment.exercise_volume_attachment will be replaced due to changes in replace_triggered_by -/+ resource \"hcloud_volume_attachment\" \"exercise_volume_attachment\" { ~ id = \"103002948\" -> (known after apply) ~ server_id = 105771291 -> (known after apply) # forces replacement # (2 unchanged attributes hidden) } # local_file.cloud_init must be replaced -/+ resource \"local_file\" \"cloud_init\" { ~ content = (sensitive value) # forces replacement ~ content_base64sha256 = \"yH7vuZ6QXui5Arp74m5Cd+UH6olTwf1FTw7+1iU0iC8=\" -> (known after apply) ~ content_base64sha512 = \"7GRH9TjHKRwYEPH5eEjC8saDFnVGGGIo7HVqjxECaFSEZUX3sWGLpSBIvxPbYd2aTXIrRSMferzdz/syClxYSQ==\" -> (known after apply) ~ content_md5 = \"0ff4cc438028c46636c28540146f5df5\" -> (known after apply) ~ content_sha1 = \"923a23b97f6dc5e555699ae9515c52886329bb01\" -> (known after apply) ~ content_sha256 = \"c87eefb99e905ee8b902ba7be26e4277e507ea8953c1fd454f0efed62534882f\" -> (known after apply) ~ content_sha512 = \"ec6447f538c7291c1810f1f97848c2f2c683167546186228ec756a8f11026854846545f7b1618ba52048bf13db61dd9a4d722b45231f7abcddcffb320a5c5849\" -> (known after apply) ~ id = \"923a23b97f6dc5e555699ae9515c52886329bb01\" -> (known after apply) # (3 unchanged attributes hidden) } Plan: 4 to add, 2 to change, 4 to destroy. Changes to Outputs: ~ server_datacenter = \"hel1-dc2\" -> (known after apply) ~ server_ip = \"95.216.223.223\" -> (known after apply) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes hcloud_volume_attachment.exercise_volume_attachment: Destroying... [id=103002948] hcloud_firewall_attachment.exercise_fw_attachment: Destroying... [id=2324824] hcloud_firewall_attachment.exercise_fw_attachment: Destruction complete after 2s hcloud_volume_attachment.exercise_volume_attachment: Destruction complete after 4s hcloud_server.exercise_20: Destroying... [id=105771291] hcloud_server.exercise_20: Destruction complete after 8s local_file.cloud_init: Destroying... [id=923a23b97f6dc5e555699ae9515c52886329bb01] local_file.cloud_init: Destruction complete after 0s local_file.cloud_init: Creating... local_file.cloud_init: Creation complete after 0s [id=6bd71fdc395ac3d3dd94c75297cae1e359bb58ad] hcloud_server.exercise_20: Creating... hcloud_server.exercise_20: Still creating... [00m10s elapsed] hcloud_server.exercise_20: Creation complete after 12s [id=105772030] hcloud_volume_attachment.exercise_volume_attachment: Creating... hcloud_firewall_attachment.exercise_fw_attachment: Creating... hcloud_firewall_attachment.exercise_fw_attachment: Creation complete after 2s [id=2324824] hcloud_volume_attachment.exercise_volume_attachment: Creation complete after 8s [id=103002948] Apply complete! Resources: 4 added, 0 changed, 4 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" volume_device = \"/dev/disk/by-id/scsi-0HC_Volume_103002948\" dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ $ ./bin/ssh.sh Linux exercise-20 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. devops@exercise-20:~$ openssl rsa -in /etc/nginx/ssl/private.pem -noout -modulus | openssl md5 MD5(stdin)= 9b486c6d6d539a37e3b152cfbfcd1b9b devops@exercise-20:~$ openssl x509 -in /etc/nginx/ssl/.pem -noout -modulus | openssl md5 certificate.pem private.pem devops@exercise-20:~$ openssl x509 -in /etc/nginx/ssl/.pem -noout -modulus | openssl md5 certificate.pem private.pem devops@exercise-20:~$ openssl x509 -in /etc/nginx/ssl/certificate.pem -noout -modulus | openssl md5 MD5(stdin)= 9b486c6d6d539a37e3b152cfbfcd1b9b devops@exercise-20:~$ sudo nano /etc/nginx/sites-available/default devops@exercise-20:~$ sudo nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful devops@exercise-20:~$ systemctl restart nginx Failed to execute /usr/bin/pkttyagent: No such file or directory Failed to restart nginx.service: Access denied See system logs and 'systemctl status nginx.service' for details. devops@exercise-20:~$ sudo systemctl restart nginx devops@exercise-20:~$ sudo systemctl status nginx \u25cf nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; preset: enabled) Active: active (running) since Fri 2025-08-01 14:20:14 UTC; 6s ago Docs: man:nginx(8) Process: 2517 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Process: 2518 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Main PID: 2519 (nginx) Tasks: 3 (limit: 2250) Memory: 2.7M CPU: 15ms CGroup: /system.slice/nginx.service \u251c\u25002519 \"nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\" \u251c\u25002520 \"nginx: worker process\" \u2514\u25002521 \"nginx: worker process\" Aug 01 14:20:14 exercise-20 systemd[1]: Starting nginx.service - A high performance web server and a reverse proxy server... Aug 01 14:20:14 exercise-20 systemd[1]: Started nginx.service - A high performance web server and a reverse proxy server. devops@exercise-20:~$ exit logout Connection to workhorse.g2.sdi.hdm-stuttgart.cloud closed. dwalz@dwolf:/mnt/d/Development/hdm/ss25/sdi/exercise22/configuration$ ./bin/ssh.sh Linux exercise-20 6.1.0-37-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.140-1 (2025-05-22) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. Last login: Fri Aug 1 14:16:22 2025 from 91.16.131.250 devops@exercise-20:~$ cat / bin/ dev/ home/ initrd.img.old lib64/ media/ opt/ root/ sbin/ sys/ usr/ vmlinuz volume01/ boot/ etc/ initrd.img lib/ lost+found/ mnt/ proc/ run/ srv/ tmp/ var/ vmlinuz.old devops@exercise-20:~$ cat / bin/ dev/ home/ initrd.img.old lib64/ media/ opt/ root/ sbin/ sys/ usr/ vmlinuz volume01/ boot/ etc/ initrd.img lib/ lost+found/ mnt/ proc/ run/ srv/ tmp/ var/ vmlinuz.old devops@exercise-20:~$ cat /etc/nginx/sites-available/default","title":"}"},{"location":"exercise22/#_14","text":"","title":""},{"location":"exercise22/#you-should-look-at-the-following-urls-in-order-to-grasp-a-solid-understanding_1","text":"","title":"You should look at the following URL's in order to grasp a solid understanding"},{"location":"exercise22/#of-nginx-configuration-files-in-order-to-fully-unleash-the-power-of-nginx_1","text":"","title":"of Nginx configuration files in order to fully unleash the power of Nginx."},{"location":"exercise22/#httpswwwnginxcomresourceswikistart_1","text":"","title":"https://www.nginx.com/resources/wiki/start/"},{"location":"exercise22/#httpswwwnginxcomresourceswikistarttopicstutorialsconfig_pitfalls_1","text":"","title":"https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/"},{"location":"exercise22/#httpswikidebianorgnginxdirectorystructure_1","text":"","title":"https://wiki.debian.org/Nginx/DirectoryStructure"},{"location":"exercise22/#_15","text":"","title":""},{"location":"exercise22/#in-most-cases-administrators-will-remove-this-file-from-sites-enabled-and_1","text":"","title":"In most cases, administrators will remove this file from sites-enabled/ and"},{"location":"exercise22/#leave-it-as-reference-inside-of-sites-available-where-it-will-continue-to-be_1","text":"","title":"leave it as reference inside of sites-available where it will continue to be"},{"location":"exercise22/#updated-by-the-nginx-packaging-team_1","text":"","title":"updated by the nginx packaging team."},{"location":"exercise22/#_16","text":"","title":""},{"location":"exercise22/#this-file-will-automatically-load-configuration-files-provided-by-other_1","text":"","title":"This file will automatically load configuration files provided by other"},{"location":"exercise22/#applications-such-as-drupal-or-wordpress-these-applications-will-be-made_1","text":"","title":"applications, such as Drupal or Wordpress. These applications will be made"},{"location":"exercise22/#available-underneath-a-path-with-that-package-name-such-as-drupal8_1","text":"","title":"available underneath a path with that package name, such as /drupal8."},{"location":"exercise22/#_17","text":"","title":""},{"location":"exercise22/#please-see-usrsharedocnginx-docexamples-for-more-detailed-examples_1","text":"","title":"Please see /usr/share/doc/nginx-doc/examples/ for more detailed examples."},{"location":"exercise22/#_18","text":"","title":""},{"location":"exercise22/#default-server-configuration_1","text":"","title":"Default server configuration"},{"location":"exercise22/#_19","text":"server { listen 80 default_server; listen [::]:80 default_server; # SSL configuration # listen 443 ssl default_server; listen [::]:443 ssl default_server; ssl_certificate /etc/nginx/ssl/certificate.pem; ssl_certificate_key /etc/nginx/ssl/private.pem; # # Note: You should disable gzip for SSL traffic. # See: https://bugs.debian.org/773332 # # Read up on ssl_ciphers to ensure a secure configuration. # See: https://bugs.debian.org/765782 # # Self signed certs generated by the ssl-cert package # Don't use them in a production server! # # include snippets/snakeoil.conf; root /var/www/html; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; } # pass PHP scripts to FastCGI server # #location ~ \\.php$ { # include snippets/fastcgi-php.conf; # # # With php-fpm (or other unix sockets): # fastcgi_pass unix:/run/php/php7.4-fpm.sock; # # With php-cgi (or other tcp sockets): # fastcgi_pass 127.0.0.1:9000; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} }","title":""},{"location":"exercise22/#virtual-host-configuration-for-examplecom_1","text":"","title":"Virtual Host configuration for example.com"},{"location":"exercise22/#_20","text":"","title":""},{"location":"exercise22/#you-can-move-that-to-a-different-file-under-sites-available-and-symlink-that_1","text":"","title":"You can move that to a different file under sites-available/ and symlink that"},{"location":"exercise22/#to-sites-enabled-to-enable-it_1","text":"","title":"to sites-enabled/ to enable it."},{"location":"exercise22/#_21","text":"","title":""},{"location":"exercise22/#server_1","text":"","title":"server {"},{"location":"exercise22/#listen-80_2","text":"","title":"listen 80;"},{"location":"exercise22/#listen-80_3","text":"","title":"listen [::]:80;"},{"location":"exercise22/#_22","text":"","title":""},{"location":"exercise22/#server_name-examplecom_1","text":"","title":"server_name example.com;"},{"location":"exercise22/#_23","text":"","title":""},{"location":"exercise22/#root-varwwwexamplecom_1","text":"","title":"root /var/www/example.com;"},{"location":"exercise22/#index-indexhtml_1","text":"","title":"index index.html;"},{"location":"exercise22/#_24","text":"","title":""},{"location":"exercise22/#location_1","text":"","title":"location / {"},{"location":"exercise22/#try_files-uri-uri-404_1","text":"","title":"try_files $uri $uri/ =404;"},{"location":"exercise22/#_25","text":"","title":"}"},{"location":"exercise22/#_26","text":"devops@exercise-20:~$","title":"}"},{"location":"exercise23/","text":"Exercise 23: Testing your web certificate Click here to view the solution in the repository. Note: The exercises 22 to 24 have been solved inside one configuration We now want to apply the received certificate to the nginx server of our configuration in order to be able to connect using HTTPS. This has been done manually before in Exercise 18 . Now instead of doing it by hand and letting certbot handle the setup and modification of the nginx configuration, it is done using cloud-init to provision the server with the content. With the write_files section both the certificate and certificate key files are transferred to the server. ${yamlencode({ write_files = [{ path = \"/etc/nginx/ssl/private.pem\" content = ssl_private }, { path = \"/etc/nginx/ssl/certificate.pem\" content = ssl_certificate }, { path = \"/etc/nginx/sites-available/default\" content = nginx_configuration }] })} Additionally a correct nginx configuration file replaces the default /etc/nginx/sites-available/default configuration. It has HTTPS enabled and references the certificate files transferred to the server: server { listen 80 default_server; listen [::]:80 default_server; # SSL configuration listen 443 ssl default_server; listen [::]:443 ssl default_server; ssl_certificate /etc/nginx/ssl/certificate.pem; ssl_certificate_key /etc/nginx/ssl/private.pem; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name _; location / { try_files $uri $uri/ =404; } } In the runcmd section the nginx service is restarted in order for the changed configuration to take effect using systemctl restart nginx . After running terraform apply to apply the configuration using the staging server first, an apply with the real server ( https://acme-v02.api.letsencrypt.org/directory ) produces a valid TLS certificate. It can be inspected in most browsers on the left of the search bar:","title":"Exercise 23"},{"location":"exercise23/#exercise-23-testing-your-web-certificate","text":"Click here to view the solution in the repository. Note: The exercises 22 to 24 have been solved inside one configuration We now want to apply the received certificate to the nginx server of our configuration in order to be able to connect using HTTPS. This has been done manually before in Exercise 18 . Now instead of doing it by hand and letting certbot handle the setup and modification of the nginx configuration, it is done using cloud-init to provision the server with the content. With the write_files section both the certificate and certificate key files are transferred to the server. ${yamlencode({ write_files = [{ path = \"/etc/nginx/ssl/private.pem\" content = ssl_private }, { path = \"/etc/nginx/ssl/certificate.pem\" content = ssl_certificate }, { path = \"/etc/nginx/sites-available/default\" content = nginx_configuration }] })} Additionally a correct nginx configuration file replaces the default /etc/nginx/sites-available/default configuration. It has HTTPS enabled and references the certificate files transferred to the server: server { listen 80 default_server; listen [::]:80 default_server; # SSL configuration listen 443 ssl default_server; listen [::]:443 ssl default_server; ssl_certificate /etc/nginx/ssl/certificate.pem; ssl_certificate_key /etc/nginx/ssl/private.pem; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name _; location / { try_files $uri $uri/ =404; } } In the runcmd section the nginx service is restarted in order for the changed configuration to take effect using systemctl restart nginx . After running terraform apply to apply the configuration using the staging server first, an apply with the real server ( https://acme-v02.api.letsencrypt.org/directory ) produces a valid TLS certificate. It can be inspected in most browsers on the left of the search bar:","title":"Exercise 23: Testing your web certificate"},{"location":"exercise24/","text":"Exercise 24: Combining certificate generation and server creation This exercise has already been implemented in the two last exercises ( Exercise 22 and Exercise23 ). The content has been added to the ongoing overall configuration. Click here to view the solution in the repository. Note: The exercises 22 to 24 have been solved inside one configuration","title":"Exercise 24"},{"location":"exercise24/#exercise-24-combining-certificate-generation-and-server-creation","text":"This exercise has already been implemented in the two last exercises ( Exercise 22 and Exercise23 ). The content has been added to the ongoing overall configuration. Click here to view the solution in the repository. Note: The exercises 22 to 24 have been solved inside one configuration","title":"Exercise 24: Combining certificate generation and server creation"},{"location":"exercise25/","text":"","title":"Exercise25"},{"location":"exercise26/","text":"","title":"Exercise26"}]}